{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3NvSJPGChFn"
      },
      "source": [
        "# Research on botnet attacks:\n",
        "\n",
        "![alt text](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQBnTCcjUdfXOdNKziYJB4KcuntahlDQ-4f4jPVboMOW7a-nmeY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2R2IQkUmnNF"
      },
      "source": [
        "## VAE on data\n",
        "\n",
        "Distinguish between normal and anomalous data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD9Qx1hJmm2o"
      },
      "source": [
        " ## 9 classes model.\n",
        " \n",
        "The first model only accounts 9 classes of anomalies, which are:\n",
        "\n",
        "- Backdoor\t\n",
        "- DoS\t\n",
        "- Exploits\t\n",
        "- Fuzzers\t\n",
        "- Generic\t\n",
        "- Normal\t\n",
        "- Reconnaissance\t\n",
        "- Shellcode\t\n",
        "- Worms\n",
        "\n",
        "Accuracy reached between the 90 and the 98%\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "Epoch Number | Accuracy Train | Accuracy Validation\n",
        "---|---|---\n",
        "20|0.9697|0.9706\n",
        "40|0.9532|0.9743\n",
        "80|0.9724|0.9736\n",
        "160|0.9546|0.9575\n",
        "\n",
        "We need to improve feature engineering!\n",
        "\n",
        "### Link to data repo:\n",
        "\n",
        "[Here](https://cloudstor.aarnet.edu.au/plus/index.php/s/2DhnLGDdEECo4ys) one can find the official repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgusqVEEIBm4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "608322cb-fd1e-461d-ef8a-4dba255dc9b5"
      },
      "source": [
        "!wget \"https://cloudstor.aarnet.edu.au/plus/s/2DhnLGDdEECo4ys/download?path=%2FUNSW-NB15%20-%20CSV%20Files%2Fa%20part%20of%20training%20and%20testing%20set&files=UNSW_NB15_training-set.csv\"\n",
        "!wget \"https://cloudstor.aarnet.edu.au/plus/s/2DhnLGDdEECo4ys/download?path=%2FUNSW-NB15%20-%20CSV%20Files%2Fa%20part%20of%20training%20and%20testing%20set&files=UNSW_NB15_testing-set.csv\"\n",
        "!wget \"https://cloudstor.aarnet.edu.au/plus/s/2DhnLGDdEECo4ys/download?path=%2FUNSW-NB15%20-%20CSV%20Files&files=UNSW-NB15_1.csv\"\n",
        "!wget \"https://cloudstor.aarnet.edu.au/plus/s/2DhnLGDdEECo4ys/download?path=%2FUNSW-NB15%20-%20CSV%20Files&files=UNSW-NB15_2.csv\"\n",
        "!wget \"https://cloudstor.aarnet.edu.au/plus/s/2DhnLGDdEECo4ys/download?path=%2FUNSW-NB15%20-%20CSV%20Files&files=UNSW-NB15_3.csv\"\n",
        "!wget \"https://cloudstor.aarnet.edu.au/plus/s/2DhnLGDdEECo4ys/download?path=%2FUNSW-NB15%20-%20CSV%20Files&files=UNSW-NB15_4.csv\"\n",
        "!wget \"https://cloudstor.aarnet.edu.au/plus/s/2DhnLGDdEECo4ys/download?path=%2FUNSW-NB15%20-%20CSV%20Files&files=UNSW-NB15_LIST_EVENTS.csv\"\n",
        "!wget \"https://cloudstor.aarnet.edu.au/plus/s/2DhnLGDdEECo4ys/download?path=%2FUNSW-NB15%20-%20CSV%20Files&files=NUSW-NB15_GT.csv\"\n",
        "!wget \"https://cloudstor.aarnet.edu.au/plus/s/2DhnLGDdEECo4ys/download?path=%2FUNSW-NB15%20-%20CSV%20Files&files=NUSW-NB15_features.csv\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-24 17:17:17--  https://cloudstor.aarnet.edu.au/plus/s/2DhnLGDdEECo4ys/download?path=%2FUNSW-NB15%20-%20CSV%20Files%2Fa%20part%20of%20training%20and%20testing%20set&files=UNSW_NB15_training-set.csv\n",
            "Resolving cloudstor.aarnet.edu.au (cloudstor.aarnet.edu.au)... 202.158.207.20\n",
            "Connecting to cloudstor.aarnet.edu.au (cloudstor.aarnet.edu.au)|202.158.207.20|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15380800 (15M) [text/csv]\n",
            "Saving to: ‘download?path=%2FUNSW-NB15 - CSV Files%2Fa part of training and testing set&files=UNSW_NB15_training-set.csv’\n",
            "\n",
            "download?path=%2FUN 100%[===================>]  14.67M  5.38MB/s    in 2.7s    \n",
            "\n",
            "2020-06-24 17:17:21 (5.38 MB/s) - ‘download?path=%2FUNSW-NB15 - CSV Files%2Fa part of training and testing set&files=UNSW_NB15_training-set.csv’ saved [15380800/15380800]\n",
            "\n",
            "--2020-06-24 17:17:22--  https://cloudstor.aarnet.edu.au/plus/s/2DhnLGDdEECo4ys/download?path=%2FUNSW-NB15%20-%20CSV%20Files%2Fa%20part%20of%20training%20and%20testing%20set&files=UNSW_NB15_testing-set.csv\n",
            "Resolving cloudstor.aarnet.edu.au (cloudstor.aarnet.edu.au)... 202.158.207.20\n",
            "Connecting to cloudstor.aarnet.edu.au (cloudstor.aarnet.edu.au)|202.158.207.20|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 32293018 (31M) [text/csv]\n",
            "Saving to: ‘download?path=%2FUNSW-NB15 - CSV Files%2Fa part of training and testing set&files=UNSW_NB15_testing-set.csv’\n",
            "\n",
            "download?path=%2FUN 100%[===================>]  30.80M  11.2MB/s    in 2.8s    \n",
            "\n",
            "2020-06-24 17:17:26 (11.2 MB/s) - ‘download?path=%2FUNSW-NB15 - CSV Files%2Fa part of training and testing set&files=UNSW_NB15_testing-set.csv’ saved [32293018/32293018]\n",
            "\n",
            "--2020-06-24 17:17:27--  https://cloudstor.aarnet.edu.au/plus/s/2DhnLGDdEECo4ys/download?path=%2FUNSW-NB15%20-%20CSV%20Files&files=UNSW-NB15_1.csv\n",
            "Resolving cloudstor.aarnet.edu.au (cloudstor.aarnet.edu.au)... 202.158.207.20\n",
            "Connecting to cloudstor.aarnet.edu.au (cloudstor.aarnet.edu.au)|202.158.207.20|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 168979718 (161M) [text/csv]\n",
            "Saving to: ‘download?path=%2FUNSW-NB15 - CSV Files&files=UNSW-NB15_1.csv’\n",
            "\n",
            "download?path=%2FUN 100%[===================>] 161.15M  18.2MB/s    in 9.9s    \n",
            "\n",
            "2020-06-24 17:17:38 (16.3 MB/s) - ‘download?path=%2FUNSW-NB15 - CSV Files&files=UNSW-NB15_1.csv’ saved [168979718/168979718]\n",
            "\n",
            "--2020-06-24 17:17:39--  https://cloudstor.aarnet.edu.au/plus/s/2DhnLGDdEECo4ys/download?path=%2FUNSW-NB15%20-%20CSV%20Files&files=UNSW-NB15_2.csv\n",
            "Resolving cloudstor.aarnet.edu.au (cloudstor.aarnet.edu.au)... 202.158.207.20\n",
            "Connecting to cloudstor.aarnet.edu.au (cloudstor.aarnet.edu.au)|202.158.207.20|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 165221021 (158M) [text/csv]\n",
            "Saving to: ‘download?path=%2FUNSW-NB15 - CSV Files&files=UNSW-NB15_2.csv’\n",
            "\n",
            "download?path=%2FUN 100%[===================>] 157.57M  11.9MB/s    in 13s     \n",
            "\n",
            "2020-06-24 17:17:54 (11.8 MB/s) - ‘download?path=%2FUNSW-NB15 - CSV Files&files=UNSW-NB15_2.csv’ saved [165221021/165221021]\n",
            "\n",
            "--2020-06-24 17:17:55--  https://cloudstor.aarnet.edu.au/plus/s/2DhnLGDdEECo4ys/download?path=%2FUNSW-NB15%20-%20CSV%20Files&files=UNSW-NB15_3.csv\n",
            "Resolving cloudstor.aarnet.edu.au (cloudstor.aarnet.edu.au)... 202.158.207.20\n",
            "Connecting to cloudstor.aarnet.edu.au (cloudstor.aarnet.edu.au)|202.158.207.20|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 154588103 (147M) [text/csv]\n",
            "Saving to: ‘download?path=%2FUNSW-NB15 - CSV Files&files=UNSW-NB15_3.csv’\n",
            "\n",
            "download?path=%2FUN 100%[===================>] 147.43M  4.81MB/s    in 31s     \n",
            "\n",
            "2020-06-24 17:18:27 (4.78 MB/s) - ‘download?path=%2FUNSW-NB15 - CSV Files&files=UNSW-NB15_3.csv’ saved [154588103/154588103]\n",
            "\n",
            "--2020-06-24 17:18:28--  https://cloudstor.aarnet.edu.au/plus/s/2DhnLGDdEECo4ys/download?path=%2FUNSW-NB15%20-%20CSV%20Files&files=UNSW-NB15_4.csv\n",
            "Resolving cloudstor.aarnet.edu.au (cloudstor.aarnet.edu.au)... 202.158.207.20\n",
            "Connecting to cloudstor.aarnet.edu.au (cloudstor.aarnet.edu.au)|202.158.207.20|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 97588754 (93M) [text/csv]\n",
            "Saving to: ‘download?path=%2FUNSW-NB15 - CSV Files&files=UNSW-NB15_4.csv’\n",
            "\n",
            "download?path=%2FUN 100%[===================>]  93.07M  4.81MB/s    in 19s     \n",
            "\n",
            "2020-06-24 17:18:49 (4.78 MB/s) - ‘download?path=%2FUNSW-NB15 - CSV Files&files=UNSW-NB15_4.csv’ saved [97588754/97588754]\n",
            "\n",
            "--2020-06-24 17:18:50--  https://cloudstor.aarnet.edu.au/plus/s/2DhnLGDdEECo4ys/download?path=%2FUNSW-NB15%20-%20CSV%20Files&files=UNSW-NB15_LIST_EVENTS.csv\n",
            "Resolving cloudstor.aarnet.edu.au (cloudstor.aarnet.edu.au)... 202.158.207.20\n",
            "Connecting to cloudstor.aarnet.edu.au (cloudstor.aarnet.edu.au)|202.158.207.20|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4639 (4.5K) [text/csv]\n",
            "Saving to: ‘download?path=%2FUNSW-NB15 - CSV Files&files=UNSW-NB15_LIST_EVENTS.csv’\n",
            "\n",
            "download?path=%2FUN 100%[===================>]   4.53K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-06-24 17:18:51 (526 MB/s) - ‘download?path=%2FUNSW-NB15 - CSV Files&files=UNSW-NB15_LIST_EVENTS.csv’ saved [4639/4639]\n",
            "\n",
            "--2020-06-24 17:18:52--  https://cloudstor.aarnet.edu.au/plus/s/2DhnLGDdEECo4ys/download?path=%2FUNSW-NB15%20-%20CSV%20Files&files=NUSW-NB15_GT.csv\n",
            "Resolving cloudstor.aarnet.edu.au (cloudstor.aarnet.edu.au)... 202.158.207.20\n",
            "Connecting to cloudstor.aarnet.edu.au (cloudstor.aarnet.edu.au)|202.158.207.20|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 86426111 (82M) [text/csv]\n",
            "Saving to: ‘download?path=%2FUNSW-NB15 - CSV Files&files=NUSW-NB15_GT.csv’\n",
            "\n",
            "download?path=%2FUN 100%[===================>]  82.42M  18.0MB/s    in 5.7s    \n",
            "\n",
            "2020-06-24 17:18:59 (14.5 MB/s) - ‘download?path=%2FUNSW-NB15 - CSV Files&files=NUSW-NB15_GT.csv’ saved [86426111/86426111]\n",
            "\n",
            "--2020-06-24 17:19:01--  https://cloudstor.aarnet.edu.au/plus/s/2DhnLGDdEECo4ys/download?path=%2FUNSW-NB15%20-%20CSV%20Files&files=NUSW-NB15_features.csv\n",
            "Resolving cloudstor.aarnet.edu.au (cloudstor.aarnet.edu.au)... 202.158.207.20\n",
            "Connecting to cloudstor.aarnet.edu.au (cloudstor.aarnet.edu.au)|202.158.207.20|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4044 (3.9K) [text/csv]\n",
            "Saving to: ‘download?path=%2FUNSW-NB15 - CSV Files&files=NUSW-NB15_features.csv’\n",
            "\n",
            "download?path=%2FUN 100%[===================>]   3.95K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-06-24 17:19:02 (645 MB/s) - ‘download?path=%2FUNSW-NB15 - CSV Files&files=NUSW-NB15_features.csv’ saved [4044/4044]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5uBhl8HC0Pf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "44d13a66-1c10-4e05-f051-f11ce28e1cdd"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import psutil\n",
        "import humanize\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.layers import Flatten,BatchNormalization,LSTM, Dense, Dropout,Conv1D,MaxPooling1D,Input,Reshape, Add, Activation, ZeroPadding1D, AveragePooling1D, Embedding\n",
        "from keras.models import Sequential, Model\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.layers.embeddings import Embedding\n",
        "import keras.backend as K\n",
        "from keras.callbacks import History\n",
        "from keras.optimizers import Adam,SGD,Adadelta,RMSprop\n",
        "from keras.regularizers import l2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import re\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import TruncatedSVD as tsvd\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy import stats as st\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKnz6GkoUQQ8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "outputId": "d8e86749-d219-4a27-9495-a576946151cf"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'download?path=%2FUNSW-NB15 - CSV Files'\n",
            "'download?path=%2FUNSW-NB15 - CSV Files.1'\n",
            "'download?path=%2FUNSW-NB15 - CSV Files.2'\n",
            "'download?path=%2FUNSW-NB15 - CSV Files%2Fa part of training and testing set'\n",
            "'download?path=%2FUNSW-NB15 - CSV Files%2Fa part of training and testing set.1'\n",
            "'download?path=%2FUNSW-NB15 - CSV Files%2Fa part of training and testing set&files=UNSW_NB15_testing-set.csv'\n",
            "'download?path=%2FUNSW-NB15 - CSV Files%2Fa part of training and testing set&files=UNSW_NB15_training-set.csv'\n",
            "'download?path=%2FUNSW-NB15 - CSV Files.3'\n",
            "'download?path=%2FUNSW-NB15 - CSV Files.4'\n",
            "'download?path=%2FUNSW-NB15 - CSV Files.5'\n",
            "'download?path=%2FUNSW-NB15 - CSV Files.6'\n",
            "'download?path=%2FUNSW-NB15 - CSV Files&files=NUSW-NB15_features.csv'\n",
            "'download?path=%2FUNSW-NB15 - CSV Files&files=NUSW-NB15_GT.csv'\n",
            "'download?path=%2FUNSW-NB15 - CSV Files&files=UNSW-NB15_1.csv'\n",
            "'download?path=%2FUNSW-NB15 - CSV Files&files=UNSW-NB15_2.csv'\n",
            "'download?path=%2FUNSW-NB15 - CSV Files&files=UNSW-NB15_3.csv'\n",
            "'download?path=%2FUNSW-NB15 - CSV Files&files=UNSW-NB15_4.csv'\n",
            "'download?path=%2FUNSW-NB15 - CSV Files&files=UNSW-NB15_LIST_EVENTS.csv'\n",
            " sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjPlxBDkJdSi"
      },
      "source": [
        "pd.options.display.max_rows = 100000\n",
        "pd.options.display.max_columns = 100000\n",
        "np.set_printoptions(threshold=sys.maxsize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vim_z1NsImAl"
      },
      "source": [
        "files_data = [file for file in os.listdir() if file.endswith('.csv')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7p3NQcVmAE5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "outputId": "c271d641-8d80-4a81-b8f4-f308c89883fc"
      },
      "source": [
        "files_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1gaFTVGI8gX"
      },
      "source": [
        "def data_loader(file_name):\n",
        "  try:\n",
        "    df_list = []\n",
        "    for file_n in file_name:\n",
        "      locals()[file_n.replace('.csv','')] = pd.read_csv(file)\n",
        "      df_list.append(locals()[file_n.replace('.csv','')])\n",
        "  except:\n",
        "    raise Exception(\"Warning, file {} has generated the following error {}\".format(file_n,str(sys.exc_info())))\n",
        "  return df_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFPbAUDw16WB"
      },
      "source": [
        "def df_shape(df):\n",
        "  for d in df:\n",
        "    print('Shape of {} is {} and has missing values:\\n{}\\n##############################\\n'.format(d,globals()[d].shape,globals()[d].isna().sum()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClSsnNfhC1BS"
      },
      "source": [
        "file_names=[el for el in os.listdir() if el.endswith(\".csv\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_jLUtbwC0X7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "outputId": "343acd97-c355-4363-fef9-3c2c6bd39f10"
      },
      "source": [
        "file_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['UNSW-NB15_3.csv',\n",
              " 'UNSW-NB15_4.csv',\n",
              " 'UNSW_NB15_testing-set.csv',\n",
              " 'UNSW-NB15_1.csv',\n",
              " 'UNSW-NB15_LIST_EVENTS.csv',\n",
              " 'UNSW_NB15_training-set.csv',\n",
              " 'NUSW-NB15_features.csv',\n",
              " 'UNSW-NB15_2.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfdxHOXCEshL"
      },
      "source": [
        "pd.read_csv('UNSW_NB15_training-set.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtRNQvV4DN3k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "outputId": "4ce0c67f-0343-4b53-8f0d-5a278d86b7c0"
      },
      "source": [
        "data_list = data_loader(file_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-2f70c484ae05>\u001b[0m in \u001b[0;36mdata_loader\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile_n\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m       \u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m       \u001b[0mdf_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: I/O operation on closed file.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-cb262ea75bb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-31-2f70c484ae05>\u001b[0m in \u001b[0;36mdata_loader\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0mdf_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Warning, file {} has generated the following error {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_n\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdf_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Warning, file UNSW-NB15_3.csv has generated the following error (<class 'ValueError'>, ValueError('I/O operation on closed file.',), <traceback object at 0x7f382bd5eac8>)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9TYzUXRmiXH"
      },
      "source": [
        "df_train = pd.read_csv(files_data[files_data.index('UNSW_NB15_training-set.csv')])\n",
        "df_test = pd.read_csv(files_data[files_data.index('UNSW_NB15_testing-set.csv')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKMZ-QBmCalX"
      },
      "source": [
        "- '0x000b'\n",
        "- '0xc0a8'\n",
        "- '0x000b'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMvaRtcQJVoc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "outputId": "dec49bad-15a7-46cc-afb8-7dd585818584"
      },
      "source": [
        "df_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(175341, 45)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsEJRItRJJPT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "b84fcc81-536c-4666-8d05-7117feceee15"
      },
      "source": [
        "data = pd.concat([df_test,df_train])\n",
        "data.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 257673 entries, 0 to 175340\n",
            "Data columns (total 45 columns):\n",
            "id                   257673 non-null int64\n",
            "dur                  257673 non-null float64\n",
            "proto                257673 non-null object\n",
            "service              257673 non-null object\n",
            "state                257673 non-null object\n",
            "spkts                257673 non-null int64\n",
            "dpkts                257673 non-null int64\n",
            "sbytes               257673 non-null int64\n",
            "dbytes               257673 non-null int64\n",
            "rate                 257673 non-null float64\n",
            "sttl                 257673 non-null int64\n",
            "dttl                 257673 non-null int64\n",
            "sload                257673 non-null float64\n",
            "dload                257673 non-null float64\n",
            "sloss                257673 non-null int64\n",
            "dloss                257673 non-null int64\n",
            "sinpkt               257673 non-null float64\n",
            "dinpkt               257673 non-null float64\n",
            "sjit                 257673 non-null float64\n",
            "djit                 257673 non-null float64\n",
            "swin                 257673 non-null int64\n",
            "stcpb                257673 non-null int64\n",
            "dtcpb                257673 non-null int64\n",
            "dwin                 257673 non-null int64\n",
            "tcprtt               257673 non-null float64\n",
            "synack               257673 non-null float64\n",
            "ackdat               257673 non-null float64\n",
            "smean                257673 non-null int64\n",
            "dmean                257673 non-null int64\n",
            "trans_depth          257673 non-null int64\n",
            "response_body_len    257673 non-null int64\n",
            "ct_srv_src           257673 non-null int64\n",
            "ct_state_ttl         257673 non-null int64\n",
            "ct_dst_ltm           257673 non-null int64\n",
            "ct_src_dport_ltm     257673 non-null int64\n",
            "ct_dst_sport_ltm     257673 non-null int64\n",
            "ct_dst_src_ltm       257673 non-null int64\n",
            "is_ftp_login         257673 non-null int64\n",
            "ct_ftp_cmd           257673 non-null int64\n",
            "ct_flw_http_mthd     257673 non-null int64\n",
            "ct_src_ltm           257673 non-null int64\n",
            "ct_srv_dst           257673 non-null int64\n",
            "is_sm_ips_ports      257673 non-null int64\n",
            "attack_cat           257673 non-null object\n",
            "label                257673 non-null int64\n",
            "dtypes: float64(11), int64(30), object(4)\n",
            "memory usage: 90.4+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItSEOR6PJoGl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7ce71bc3-3edc-45c5-da7d-7de0bc031ef4"
      },
      "source": [
        "data.loc[(data.attack_cat == 'Worms')&(data.label == 0),].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 45)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e_2V9-4Q3E5"
      },
      "source": [
        "## Working on data:\n",
        "\n",
        "- Get the text data into variables. We will do that by inserting feature selection, but in a second moment;\n",
        "-  For now, we will go with LabelEncoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfpRnzLAUewR"
      },
      "source": [
        "data = data.drop_duplicates(keep='first',)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exJMlynp-eh3"
      },
      "source": [
        "data = data.reset_index()\n",
        "data = data.drop(['index','id'],axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6EtPDoG-lnp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b83d6e06-bc30-48e5-f6eb-fd7f7341d21c"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(257673, 44)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrXyiuMuS2ch"
      },
      "source": [
        "tgt0 = data.attack_cat\n",
        "tgt2 = data.label\n",
        "data.drop(['attack_cat','label'],axis=1,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci4NaQAthoqC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7f4d5a14-e00e-4e15-d846-da5807a833be"
      },
      "source": [
        "tgt0.unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Normal', 'Reconnaissance', 'Backdoor', 'DoS', 'Exploits',\n",
              "       'Analysis', 'Fuzzers', 'Worms', 'Shellcode', 'Generic'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-E9Xdn5Cb85J"
      },
      "source": [
        "-------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBEBg-nkJJZa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "a0e383a4-8301-4b9b-ff15-e3aeb73bd46a"
      },
      "source": [
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dur</th>\n",
              "      <th>spkts</th>\n",
              "      <th>dpkts</th>\n",
              "      <th>sbytes</th>\n",
              "      <th>dbytes</th>\n",
              "      <th>rate</th>\n",
              "      <th>sttl</th>\n",
              "      <th>dttl</th>\n",
              "      <th>sload</th>\n",
              "      <th>dload</th>\n",
              "      <th>sloss</th>\n",
              "      <th>dloss</th>\n",
              "      <th>sinpkt</th>\n",
              "      <th>dinpkt</th>\n",
              "      <th>sjit</th>\n",
              "      <th>djit</th>\n",
              "      <th>swin</th>\n",
              "      <th>stcpb</th>\n",
              "      <th>dtcpb</th>\n",
              "      <th>dwin</th>\n",
              "      <th>tcprtt</th>\n",
              "      <th>synack</th>\n",
              "      <th>ackdat</th>\n",
              "      <th>smean</th>\n",
              "      <th>dmean</th>\n",
              "      <th>trans_depth</th>\n",
              "      <th>response_body_len</th>\n",
              "      <th>ct_srv_src</th>\n",
              "      <th>ct_state_ttl</th>\n",
              "      <th>ct_dst_ltm</th>\n",
              "      <th>ct_src_dport_ltm</th>\n",
              "      <th>ct_dst_sport_ltm</th>\n",
              "      <th>ct_dst_src_ltm</th>\n",
              "      <th>is_ftp_login</th>\n",
              "      <th>ct_ftp_cmd</th>\n",
              "      <th>ct_flw_http_mthd</th>\n",
              "      <th>ct_src_ltm</th>\n",
              "      <th>ct_srv_dst</th>\n",
              "      <th>is_sm_ips_ports</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>257673.000000</td>\n",
              "      <td>257673.000000</td>\n",
              "      <td>257673.000000</td>\n",
              "      <td>2.576730e+05</td>\n",
              "      <td>2.576730e+05</td>\n",
              "      <td>2.576730e+05</td>\n",
              "      <td>257673.000000</td>\n",
              "      <td>257673.000000</td>\n",
              "      <td>2.576730e+05</td>\n",
              "      <td>2.576730e+05</td>\n",
              "      <td>257673.000000</td>\n",
              "      <td>257673.000000</td>\n",
              "      <td>257673.000000</td>\n",
              "      <td>257673.000000</td>\n",
              "      <td>2.576730e+05</td>\n",
              "      <td>257673.000000</td>\n",
              "      <td>257673.000000</td>\n",
              "      <td>2.576730e+05</td>\n",
              "      <td>2.576730e+05</td>\n",
              "      <td>257673.000000</td>\n",
              "      <td>257673.000000</td>\n",
              "      <td>257673.000000</td>\n",
              "      <td>257673.000000</td>\n",
              "      <td>257673.000000</td>\n",
              "      <td>257673.000000</td>\n",
              "      <td>257673.000000</td>\n",
              "      <td>2.576730e+05</td>\n",
              "      <td>257673.000000</td>\n",
              "      <td>257673.000000</td>\n",
              "      <td>257673.000000</td>\n",
              "      <td>257673.000000</td>\n",
              "      <td>257673.000000</td>\n",
              "      <td>257673.000000</td>\n",
              "      <td>257673.000000</td>\n",
              "      <td>257673.000000</td>\n",
              "      <td>257673.000000</td>\n",
              "      <td>257673.000000</td>\n",
              "      <td>257673.000000</td>\n",
              "      <td>257673.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.246715</td>\n",
              "      <td>19.777144</td>\n",
              "      <td>18.514703</td>\n",
              "      <td>8.572952e+03</td>\n",
              "      <td>1.438729e+04</td>\n",
              "      <td>9.125391e+04</td>\n",
              "      <td>180.000931</td>\n",
              "      <td>84.754957</td>\n",
              "      <td>7.060869e+07</td>\n",
              "      <td>6.582143e+05</td>\n",
              "      <td>4.889317</td>\n",
              "      <td>6.743691</td>\n",
              "      <td>912.300834</td>\n",
              "      <td>98.915462</td>\n",
              "      <td>5.419373e+03</td>\n",
              "      <td>582.251456</td>\n",
              "      <td>121.753661</td>\n",
              "      <td>1.006120e+09</td>\n",
              "      <td>1.002295e+09</td>\n",
              "      <td>119.254629</td>\n",
              "      <td>0.046038</td>\n",
              "      <td>0.023652</td>\n",
              "      <td>0.022386</td>\n",
              "      <td>137.639027</td>\n",
              "      <td>121.649703</td>\n",
              "      <td>0.102242</td>\n",
              "      <td>1.968900e+03</td>\n",
              "      <td>9.383176</td>\n",
              "      <td>1.324978</td>\n",
              "      <td>6.050467</td>\n",
              "      <td>5.238271</td>\n",
              "      <td>4.032677</td>\n",
              "      <td>8.322964</td>\n",
              "      <td>0.012819</td>\n",
              "      <td>0.012850</td>\n",
              "      <td>0.132005</td>\n",
              "      <td>6.800045</td>\n",
              "      <td>9.121049</td>\n",
              "      <td>0.014274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>5.974305</td>\n",
              "      <td>135.947152</td>\n",
              "      <td>111.985965</td>\n",
              "      <td>1.737739e+05</td>\n",
              "      <td>1.461993e+05</td>\n",
              "      <td>1.603446e+05</td>\n",
              "      <td>102.488268</td>\n",
              "      <td>112.762131</td>\n",
              "      <td>1.857313e+08</td>\n",
              "      <td>2.412372e+06</td>\n",
              "      <td>65.574953</td>\n",
              "      <td>53.702222</td>\n",
              "      <td>6922.153239</td>\n",
              "      <td>1094.048691</td>\n",
              "      <td>4.903450e+04</td>\n",
              "      <td>3930.153369</td>\n",
              "      <td>127.367443</td>\n",
              "      <td>1.367795e+09</td>\n",
              "      <td>1.363877e+09</td>\n",
              "      <td>127.230477</td>\n",
              "      <td>0.092908</td>\n",
              "      <td>0.053856</td>\n",
              "      <td>0.045771</td>\n",
              "      <td>205.901118</td>\n",
              "      <td>254.041013</td>\n",
              "      <td>0.710593</td>\n",
              "      <td>4.962523e+04</td>\n",
              "      <td>10.829706</td>\n",
              "      <td>0.992300</td>\n",
              "      <td>8.173749</td>\n",
              "      <td>8.160822</td>\n",
              "      <td>5.831515</td>\n",
              "      <td>11.120754</td>\n",
              "      <td>0.116091</td>\n",
              "      <td>0.116421</td>\n",
              "      <td>0.681854</td>\n",
              "      <td>8.396266</td>\n",
              "      <td>10.874752</td>\n",
              "      <td>0.118618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.400000e+01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000008</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.140000e+02</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.078928e+01</td>\n",
              "      <td>62.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.231800e+04</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>57.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.004285</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.280000e+02</td>\n",
              "      <td>1.780000e+02</td>\n",
              "      <td>2.955665e+03</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>7.439423e+05</td>\n",
              "      <td>1.747441e+03</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.381696</td>\n",
              "      <td>0.007000</td>\n",
              "      <td>6.736370e-01</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>73.000000</td>\n",
              "      <td>44.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.685777</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>1.362000e+03</td>\n",
              "      <td>1.064000e+03</td>\n",
              "      <td>1.250000e+05</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>252.000000</td>\n",
              "      <td>8.000000e+07</td>\n",
              "      <td>2.210538e+04</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>58.094727</td>\n",
              "      <td>56.438859</td>\n",
              "      <td>2.787367e+03</td>\n",
              "      <td>119.712937</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>2.007375e+09</td>\n",
              "      <td>1.992752e+09</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>0.082082</td>\n",
              "      <td>0.036842</td>\n",
              "      <td>0.044665</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>89.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>59.999989</td>\n",
              "      <td>10646.000000</td>\n",
              "      <td>11018.000000</td>\n",
              "      <td>1.435577e+07</td>\n",
              "      <td>1.465753e+07</td>\n",
              "      <td>1.000000e+06</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>5.988000e+09</td>\n",
              "      <td>2.242273e+07</td>\n",
              "      <td>5319.000000</td>\n",
              "      <td>5507.000000</td>\n",
              "      <td>84371.496000</td>\n",
              "      <td>57739.240000</td>\n",
              "      <td>1.483831e+06</td>\n",
              "      <td>463199.240100</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>4.294959e+09</td>\n",
              "      <td>4.294882e+09</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>3.821465</td>\n",
              "      <td>3.226788</td>\n",
              "      <td>2.928778</td>\n",
              "      <td>1504.000000</td>\n",
              "      <td>1500.000000</td>\n",
              "      <td>172.000000</td>\n",
              "      <td>6.558056e+06</td>\n",
              "      <td>63.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>59.000000</td>\n",
              "      <td>59.000000</td>\n",
              "      <td>46.000000</td>\n",
              "      <td>65.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>62.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 dur          spkts          dpkts        sbytes  \\\n",
              "count  257673.000000  257673.000000  257673.000000  2.576730e+05   \n",
              "mean        1.246715      19.777144      18.514703  8.572952e+03   \n",
              "std         5.974305     135.947152     111.985965  1.737739e+05   \n",
              "min         0.000000       1.000000       0.000000  2.400000e+01   \n",
              "25%         0.000008       2.000000       0.000000  1.140000e+02   \n",
              "50%         0.004285       4.000000       2.000000  5.280000e+02   \n",
              "75%         0.685777      12.000000      10.000000  1.362000e+03   \n",
              "max        59.999989   10646.000000   11018.000000  1.435577e+07   \n",
              "\n",
              "             dbytes          rate           sttl           dttl         sload  \\\n",
              "count  2.576730e+05  2.576730e+05  257673.000000  257673.000000  2.576730e+05   \n",
              "mean   1.438729e+04  9.125391e+04     180.000931      84.754957  7.060869e+07   \n",
              "std    1.461993e+05  1.603446e+05     102.488268     112.762131  1.857313e+08   \n",
              "min    0.000000e+00  0.000000e+00       0.000000       0.000000  0.000000e+00   \n",
              "25%    0.000000e+00  3.078928e+01      62.000000       0.000000  1.231800e+04   \n",
              "50%    1.780000e+02  2.955665e+03     254.000000      29.000000  7.439423e+05   \n",
              "75%    1.064000e+03  1.250000e+05     254.000000     252.000000  8.000000e+07   \n",
              "max    1.465753e+07  1.000000e+06     255.000000     254.000000  5.988000e+09   \n",
              "\n",
              "              dload          sloss          dloss         sinpkt  \\\n",
              "count  2.576730e+05  257673.000000  257673.000000  257673.000000   \n",
              "mean   6.582143e+05       4.889317       6.743691     912.300834   \n",
              "std    2.412372e+06      65.574953      53.702222    6922.153239   \n",
              "min    0.000000e+00       0.000000       0.000000       0.000000   \n",
              "25%    0.000000e+00       0.000000       0.000000       0.008000   \n",
              "50%    1.747441e+03       0.000000       0.000000       0.381696   \n",
              "75%    2.210538e+04       3.000000       2.000000      58.094727   \n",
              "max    2.242273e+07    5319.000000    5507.000000   84371.496000   \n",
              "\n",
              "              dinpkt          sjit           djit           swin  \\\n",
              "count  257673.000000  2.576730e+05  257673.000000  257673.000000   \n",
              "mean       98.915462  5.419373e+03     582.251456     121.753661   \n",
              "std      1094.048691  4.903450e+04    3930.153369     127.367443   \n",
              "min         0.000000  0.000000e+00       0.000000       0.000000   \n",
              "25%         0.000000  0.000000e+00       0.000000       0.000000   \n",
              "50%         0.007000  6.736370e-01       0.000000       0.000000   \n",
              "75%        56.438859  2.787367e+03     119.712937     255.000000   \n",
              "max     57739.240000  1.483831e+06  463199.240100     255.000000   \n",
              "\n",
              "              stcpb         dtcpb           dwin         tcprtt  \\\n",
              "count  2.576730e+05  2.576730e+05  257673.000000  257673.000000   \n",
              "mean   1.006120e+09  1.002295e+09     119.254629       0.046038   \n",
              "std    1.367795e+09  1.363877e+09     127.230477       0.092908   \n",
              "min    0.000000e+00  0.000000e+00       0.000000       0.000000   \n",
              "25%    0.000000e+00  0.000000e+00       0.000000       0.000000   \n",
              "50%    0.000000e+00  0.000000e+00       0.000000       0.000000   \n",
              "75%    2.007375e+09  1.992752e+09     255.000000       0.082082   \n",
              "max    4.294959e+09  4.294882e+09     255.000000       3.821465   \n",
              "\n",
              "              synack         ackdat          smean          dmean  \\\n",
              "count  257673.000000  257673.000000  257673.000000  257673.000000   \n",
              "mean        0.023652       0.022386     137.639027     121.649703   \n",
              "std         0.053856       0.045771     205.901118     254.041013   \n",
              "min         0.000000       0.000000      24.000000       0.000000   \n",
              "25%         0.000000       0.000000      57.000000       0.000000   \n",
              "50%         0.000000       0.000000      73.000000      44.000000   \n",
              "75%         0.036842       0.044665     100.000000      89.000000   \n",
              "max         3.226788       2.928778    1504.000000    1500.000000   \n",
              "\n",
              "         trans_depth  response_body_len     ct_srv_src   ct_state_ttl  \\\n",
              "count  257673.000000       2.576730e+05  257673.000000  257673.000000   \n",
              "mean        0.102242       1.968900e+03       9.383176       1.324978   \n",
              "std         0.710593       4.962523e+04      10.829706       0.992300   \n",
              "min         0.000000       0.000000e+00       1.000000       0.000000   \n",
              "25%         0.000000       0.000000e+00       2.000000       1.000000   \n",
              "50%         0.000000       0.000000e+00       5.000000       1.000000   \n",
              "75%         0.000000       0.000000e+00      12.000000       2.000000   \n",
              "max       172.000000       6.558056e+06      63.000000       6.000000   \n",
              "\n",
              "          ct_dst_ltm  ct_src_dport_ltm  ct_dst_sport_ltm  ct_dst_src_ltm  \\\n",
              "count  257673.000000     257673.000000     257673.000000   257673.000000   \n",
              "mean        6.050467          5.238271          4.032677        8.322964   \n",
              "std         8.173749          8.160822          5.831515       11.120754   \n",
              "min         1.000000          1.000000          1.000000        1.000000   \n",
              "25%         1.000000          1.000000          1.000000        1.000000   \n",
              "50%         2.000000          1.000000          1.000000        3.000000   \n",
              "75%         6.000000          4.000000          3.000000        8.000000   \n",
              "max        59.000000         59.000000         46.000000       65.000000   \n",
              "\n",
              "        is_ftp_login     ct_ftp_cmd  ct_flw_http_mthd     ct_src_ltm  \\\n",
              "count  257673.000000  257673.000000     257673.000000  257673.000000   \n",
              "mean        0.012819       0.012850          0.132005       6.800045   \n",
              "std         0.116091       0.116421          0.681854       8.396266   \n",
              "min         0.000000       0.000000          0.000000       1.000000   \n",
              "25%         0.000000       0.000000          0.000000       2.000000   \n",
              "50%         0.000000       0.000000          0.000000       3.000000   \n",
              "75%         0.000000       0.000000          0.000000       8.000000   \n",
              "max         4.000000       4.000000         30.000000      60.000000   \n",
              "\n",
              "          ct_srv_dst  is_sm_ips_ports  \n",
              "count  257673.000000    257673.000000  \n",
              "mean        9.121049         0.014274  \n",
              "std        10.874752         0.118618  \n",
              "min         1.000000         0.000000  \n",
              "25%         2.000000         0.000000  \n",
              "50%         4.000000         0.000000  \n",
              "75%        11.000000         0.000000  \n",
              "max        62.000000         1.000000  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2IrjwSiQveY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6a268730-856a-4d12-bdae-789ce2d05ba8"
      },
      "source": [
        "obj_col=data.select_dtypes('object').columns.tolist()\n",
        "\n",
        "pd.concat([data.nunique().rename('#_uniques'),data.dtypes.rename('types')],axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>#_uniques</th>\n",
              "      <th>types</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>dur</th>\n",
              "      <td>109945</td>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>proto</th>\n",
              "      <td>133</td>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>service</th>\n",
              "      <td>13</td>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>state</th>\n",
              "      <td>11</td>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>spkts</th>\n",
              "      <td>646</td>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dpkts</th>\n",
              "      <td>627</td>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sbytes</th>\n",
              "      <td>9382</td>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dbytes</th>\n",
              "      <td>8653</td>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rate</th>\n",
              "      <td>115763</td>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sttl</th>\n",
              "      <td>13</td>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dttl</th>\n",
              "      <td>9</td>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sload</th>\n",
              "      <td>121356</td>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dload</th>\n",
              "      <td>116380</td>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sloss</th>\n",
              "      <td>490</td>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dloss</th>\n",
              "      <td>476</td>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sinpkt</th>\n",
              "      <td>114318</td>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dinpkt</th>\n",
              "      <td>110270</td>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sjit</th>\n",
              "      <td>117101</td>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>djit</th>\n",
              "      <td>114861</td>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>swin</th>\n",
              "      <td>22</td>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stcpb</th>\n",
              "      <td>114473</td>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dtcpb</th>\n",
              "      <td>114187</td>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dwin</th>\n",
              "      <td>19</td>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tcprtt</th>\n",
              "      <td>63878</td>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>synack</th>\n",
              "      <td>57366</td>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ackdat</th>\n",
              "      <td>53248</td>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smean</th>\n",
              "      <td>1377</td>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dmean</th>\n",
              "      <td>1362</td>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>trans_depth</th>\n",
              "      <td>14</td>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>response_body_len</th>\n",
              "      <td>2819</td>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ct_srv_src</th>\n",
              "      <td>57</td>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ct_state_ttl</th>\n",
              "      <td>7</td>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ct_dst_ltm</th>\n",
              "      <td>52</td>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ct_src_dport_ltm</th>\n",
              "      <td>52</td>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ct_dst_sport_ltm</th>\n",
              "      <td>35</td>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ct_dst_src_ltm</th>\n",
              "      <td>58</td>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>is_ftp_login</th>\n",
              "      <td>4</td>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ct_ftp_cmd</th>\n",
              "      <td>4</td>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ct_flw_http_mthd</th>\n",
              "      <td>11</td>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ct_src_ltm</th>\n",
              "      <td>52</td>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ct_srv_dst</th>\n",
              "      <td>57</td>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>is_sm_ips_ports</th>\n",
              "      <td>2</td>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   #_uniques    types\n",
              "dur                   109945  float64\n",
              "proto                    133   object\n",
              "service                   13   object\n",
              "state                     11   object\n",
              "spkts                    646    int64\n",
              "dpkts                    627    int64\n",
              "sbytes                  9382    int64\n",
              "dbytes                  8653    int64\n",
              "rate                  115763  float64\n",
              "sttl                      13    int64\n",
              "dttl                       9    int64\n",
              "sload                 121356  float64\n",
              "dload                 116380  float64\n",
              "sloss                    490    int64\n",
              "dloss                    476    int64\n",
              "sinpkt                114318  float64\n",
              "dinpkt                110270  float64\n",
              "sjit                  117101  float64\n",
              "djit                  114861  float64\n",
              "swin                      22    int64\n",
              "stcpb                 114473    int64\n",
              "dtcpb                 114187    int64\n",
              "dwin                      19    int64\n",
              "tcprtt                 63878  float64\n",
              "synack                 57366  float64\n",
              "ackdat                 53248  float64\n",
              "smean                   1377    int64\n",
              "dmean                   1362    int64\n",
              "trans_depth               14    int64\n",
              "response_body_len       2819    int64\n",
              "ct_srv_src                57    int64\n",
              "ct_state_ttl               7    int64\n",
              "ct_dst_ltm                52    int64\n",
              "ct_src_dport_ltm          52    int64\n",
              "ct_dst_sport_ltm          35    int64\n",
              "ct_dst_src_ltm            58    int64\n",
              "is_ftp_login               4    int64\n",
              "ct_ftp_cmd                 4    int64\n",
              "ct_flw_http_mthd          11    int64\n",
              "ct_src_ltm                52    int64\n",
              "ct_srv_dst                57    int64\n",
              "is_sm_ips_ports            2    int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyCRI-ysWh_V"
      },
      "source": [
        "tgt1 = pd.get_dummies(tgt0)\n",
        "data_new = pd.concat([pd.get_dummies(data[obj_col],drop_first=True),data[data.select_dtypes(exclude='object').columns.tolist()]],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INtN1evv7UEp"
      },
      "source": [
        "cat_col = data_new.columns[(data_new.describe().loc['max',:] == 1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tO2cKn83Zg0"
      },
      "source": [
        "cont_col = list(set(data_new.columns)-set(cat_col))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gK7nx7sUZzVd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "375cdbdf-face-427f-b9f4-6edb1a6ccc69"
      },
      "source": [
        "data_new.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(257673, 193)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz9xwA9tSnII",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "d03d9ff3-aef7-4526-88cc-407e2b0e2f0e"
      },
      "source": [
        "data_new.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>proto_a/n</th>\n",
              "      <th>proto_aes-sp3-d</th>\n",
              "      <th>proto_any</th>\n",
              "      <th>proto_argus</th>\n",
              "      <th>proto_aris</th>\n",
              "      <th>proto_arp</th>\n",
              "      <th>proto_ax.25</th>\n",
              "      <th>proto_bbn-rcc</th>\n",
              "      <th>proto_bna</th>\n",
              "      <th>proto_br-sat-mon</th>\n",
              "      <th>proto_cbt</th>\n",
              "      <th>proto_cftp</th>\n",
              "      <th>proto_chaos</th>\n",
              "      <th>proto_compaq-peer</th>\n",
              "      <th>proto_cphb</th>\n",
              "      <th>proto_cpnx</th>\n",
              "      <th>proto_crtp</th>\n",
              "      <th>proto_crudp</th>\n",
              "      <th>proto_dcn</th>\n",
              "      <th>proto_ddp</th>\n",
              "      <th>proto_ddx</th>\n",
              "      <th>proto_dgp</th>\n",
              "      <th>proto_egp</th>\n",
              "      <th>proto_eigrp</th>\n",
              "      <th>proto_emcon</th>\n",
              "      <th>proto_encap</th>\n",
              "      <th>proto_etherip</th>\n",
              "      <th>proto_fc</th>\n",
              "      <th>proto_fire</th>\n",
              "      <th>proto_ggp</th>\n",
              "      <th>proto_gmtp</th>\n",
              "      <th>proto_gre</th>\n",
              "      <th>proto_hmp</th>\n",
              "      <th>proto_i-nlsp</th>\n",
              "      <th>proto_iatp</th>\n",
              "      <th>proto_ib</th>\n",
              "      <th>proto_icmp</th>\n",
              "      <th>proto_idpr</th>\n",
              "      <th>proto_idpr-cmtp</th>\n",
              "      <th>proto_idrp</th>\n",
              "      <th>proto_ifmp</th>\n",
              "      <th>proto_igmp</th>\n",
              "      <th>proto_igp</th>\n",
              "      <th>proto_il</th>\n",
              "      <th>proto_ip</th>\n",
              "      <th>proto_ipcomp</th>\n",
              "      <th>proto_ipcv</th>\n",
              "      <th>proto_ipip</th>\n",
              "      <th>proto_iplt</th>\n",
              "      <th>proto_ipnip</th>\n",
              "      <th>proto_ippc</th>\n",
              "      <th>proto_ipv6</th>\n",
              "      <th>proto_ipv6-frag</th>\n",
              "      <th>proto_ipv6-no</th>\n",
              "      <th>proto_ipv6-opts</th>\n",
              "      <th>proto_ipv6-route</th>\n",
              "      <th>proto_ipx-n-ip</th>\n",
              "      <th>proto_irtp</th>\n",
              "      <th>proto_isis</th>\n",
              "      <th>proto_iso-ip</th>\n",
              "      <th>proto_iso-tp4</th>\n",
              "      <th>proto_kryptolan</th>\n",
              "      <th>proto_l2tp</th>\n",
              "      <th>proto_larp</th>\n",
              "      <th>proto_leaf-1</th>\n",
              "      <th>proto_leaf-2</th>\n",
              "      <th>proto_merit-inp</th>\n",
              "      <th>proto_mfe-nsp</th>\n",
              "      <th>proto_mhrp</th>\n",
              "      <th>proto_micp</th>\n",
              "      <th>proto_mobile</th>\n",
              "      <th>proto_mtp</th>\n",
              "      <th>proto_mux</th>\n",
              "      <th>proto_narp</th>\n",
              "      <th>proto_netblt</th>\n",
              "      <th>proto_nsfnet-igp</th>\n",
              "      <th>proto_nvp</th>\n",
              "      <th>proto_ospf</th>\n",
              "      <th>proto_pgm</th>\n",
              "      <th>proto_pim</th>\n",
              "      <th>proto_pipe</th>\n",
              "      <th>proto_pnni</th>\n",
              "      <th>proto_pri-enc</th>\n",
              "      <th>proto_prm</th>\n",
              "      <th>proto_ptp</th>\n",
              "      <th>proto_pup</th>\n",
              "      <th>proto_pvp</th>\n",
              "      <th>proto_qnx</th>\n",
              "      <th>proto_rdp</th>\n",
              "      <th>proto_rsvp</th>\n",
              "      <th>proto_rtp</th>\n",
              "      <th>proto_rvd</th>\n",
              "      <th>proto_sat-expak</th>\n",
              "      <th>proto_sat-mon</th>\n",
              "      <th>proto_sccopmce</th>\n",
              "      <th>proto_scps</th>\n",
              "      <th>proto_sctp</th>\n",
              "      <th>proto_sdrp</th>\n",
              "      <th>proto_secure-vmtp</th>\n",
              "      <th>proto_sep</th>\n",
              "      <th>proto_skip</th>\n",
              "      <th>proto_sm</th>\n",
              "      <th>proto_smp</th>\n",
              "      <th>proto_snp</th>\n",
              "      <th>proto_sprite-rpc</th>\n",
              "      <th>proto_sps</th>\n",
              "      <th>proto_srp</th>\n",
              "      <th>proto_st2</th>\n",
              "      <th>proto_stp</th>\n",
              "      <th>proto_sun-nd</th>\n",
              "      <th>proto_swipe</th>\n",
              "      <th>proto_tcf</th>\n",
              "      <th>proto_tcp</th>\n",
              "      <th>proto_tlsp</th>\n",
              "      <th>proto_tp++</th>\n",
              "      <th>proto_trunk-1</th>\n",
              "      <th>proto_trunk-2</th>\n",
              "      <th>proto_ttp</th>\n",
              "      <th>proto_udp</th>\n",
              "      <th>proto_unas</th>\n",
              "      <th>proto_uti</th>\n",
              "      <th>proto_vines</th>\n",
              "      <th>proto_visa</th>\n",
              "      <th>proto_vmtp</th>\n",
              "      <th>proto_vrrp</th>\n",
              "      <th>proto_wb-expak</th>\n",
              "      <th>proto_wb-mon</th>\n",
              "      <th>proto_wsn</th>\n",
              "      <th>proto_xnet</th>\n",
              "      <th>proto_xns-idp</th>\n",
              "      <th>proto_xtp</th>\n",
              "      <th>proto_zero</th>\n",
              "      <th>service_dhcp</th>\n",
              "      <th>service_dns</th>\n",
              "      <th>service_ftp</th>\n",
              "      <th>service_ftp-data</th>\n",
              "      <th>service_http</th>\n",
              "      <th>service_irc</th>\n",
              "      <th>service_pop3</th>\n",
              "      <th>service_radius</th>\n",
              "      <th>service_smtp</th>\n",
              "      <th>service_snmp</th>\n",
              "      <th>service_ssh</th>\n",
              "      <th>service_ssl</th>\n",
              "      <th>state_CLO</th>\n",
              "      <th>state_CON</th>\n",
              "      <th>state_ECO</th>\n",
              "      <th>state_FIN</th>\n",
              "      <th>state_INT</th>\n",
              "      <th>state_PAR</th>\n",
              "      <th>state_REQ</th>\n",
              "      <th>state_RST</th>\n",
              "      <th>state_URN</th>\n",
              "      <th>state_no</th>\n",
              "      <th>dur</th>\n",
              "      <th>spkts</th>\n",
              "      <th>dpkts</th>\n",
              "      <th>sbytes</th>\n",
              "      <th>dbytes</th>\n",
              "      <th>rate</th>\n",
              "      <th>sttl</th>\n",
              "      <th>dttl</th>\n",
              "      <th>sload</th>\n",
              "      <th>dload</th>\n",
              "      <th>sloss</th>\n",
              "      <th>dloss</th>\n",
              "      <th>sinpkt</th>\n",
              "      <th>dinpkt</th>\n",
              "      <th>sjit</th>\n",
              "      <th>djit</th>\n",
              "      <th>swin</th>\n",
              "      <th>stcpb</th>\n",
              "      <th>dtcpb</th>\n",
              "      <th>dwin</th>\n",
              "      <th>tcprtt</th>\n",
              "      <th>synack</th>\n",
              "      <th>ackdat</th>\n",
              "      <th>smean</th>\n",
              "      <th>dmean</th>\n",
              "      <th>trans_depth</th>\n",
              "      <th>response_body_len</th>\n",
              "      <th>ct_srv_src</th>\n",
              "      <th>ct_state_ttl</th>\n",
              "      <th>ct_dst_ltm</th>\n",
              "      <th>ct_src_dport_ltm</th>\n",
              "      <th>ct_dst_sport_ltm</th>\n",
              "      <th>ct_dst_src_ltm</th>\n",
              "      <th>is_ftp_login</th>\n",
              "      <th>ct_ftp_cmd</th>\n",
              "      <th>ct_flw_http_mthd</th>\n",
              "      <th>ct_src_ltm</th>\n",
              "      <th>ct_srv_dst</th>\n",
              "      <th>is_sm_ips_ports</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>496</td>\n",
              "      <td>0</td>\n",
              "      <td>90909.0902</td>\n",
              "      <td>254</td>\n",
              "      <td>0</td>\n",
              "      <td>180363632.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>248</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1762</td>\n",
              "      <td>0</td>\n",
              "      <td>125000.0003</td>\n",
              "      <td>254</td>\n",
              "      <td>0</td>\n",
              "      <td>881000000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>881</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1068</td>\n",
              "      <td>0</td>\n",
              "      <td>200000.0051</td>\n",
              "      <td>254</td>\n",
              "      <td>0</td>\n",
              "      <td>854400000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>534</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>900</td>\n",
              "      <td>0</td>\n",
              "      <td>166666.6608</td>\n",
              "      <td>254</td>\n",
              "      <td>0</td>\n",
              "      <td>600000000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>450</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2126</td>\n",
              "      <td>0</td>\n",
              "      <td>100000.0025</td>\n",
              "      <td>254</td>\n",
              "      <td>0</td>\n",
              "      <td>850400000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1063</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   proto_a/n  proto_aes-sp3-d  proto_any  proto_argus  proto_aris  proto_arp  \\\n",
              "0          0                0          0            0           0          0   \n",
              "1          0                0          0            0           0          0   \n",
              "2          0                0          0            0           0          0   \n",
              "3          0                0          0            0           0          0   \n",
              "4          0                0          0            0           0          0   \n",
              "\n",
              "   proto_ax.25  proto_bbn-rcc  proto_bna  proto_br-sat-mon  proto_cbt  \\\n",
              "0            0              0          0                 0          0   \n",
              "1            0              0          0                 0          0   \n",
              "2            0              0          0                 0          0   \n",
              "3            0              0          0                 0          0   \n",
              "4            0              0          0                 0          0   \n",
              "\n",
              "   proto_cftp  proto_chaos  proto_compaq-peer  proto_cphb  proto_cpnx  \\\n",
              "0           0            0                  0           0           0   \n",
              "1           0            0                  0           0           0   \n",
              "2           0            0                  0           0           0   \n",
              "3           0            0                  0           0           0   \n",
              "4           0            0                  0           0           0   \n",
              "\n",
              "   proto_crtp  proto_crudp  proto_dcn  proto_ddp  proto_ddx  proto_dgp  \\\n",
              "0           0            0          0          0          0          0   \n",
              "1           0            0          0          0          0          0   \n",
              "2           0            0          0          0          0          0   \n",
              "3           0            0          0          0          0          0   \n",
              "4           0            0          0          0          0          0   \n",
              "\n",
              "   proto_egp  proto_eigrp  proto_emcon  proto_encap  proto_etherip  proto_fc  \\\n",
              "0          0            0            0            0              0         0   \n",
              "1          0            0            0            0              0         0   \n",
              "2          0            0            0            0              0         0   \n",
              "3          0            0            0            0              0         0   \n",
              "4          0            0            0            0              0         0   \n",
              "\n",
              "   proto_fire  proto_ggp  proto_gmtp  proto_gre  proto_hmp  proto_i-nlsp  \\\n",
              "0           0          0           0          0          0             0   \n",
              "1           0          0           0          0          0             0   \n",
              "2           0          0           0          0          0             0   \n",
              "3           0          0           0          0          0             0   \n",
              "4           0          0           0          0          0             0   \n",
              "\n",
              "   proto_iatp  proto_ib  proto_icmp  proto_idpr  proto_idpr-cmtp  proto_idrp  \\\n",
              "0           0         0           0           0                0           0   \n",
              "1           0         0           0           0                0           0   \n",
              "2           0         0           0           0                0           0   \n",
              "3           0         0           0           0                0           0   \n",
              "4           0         0           0           0                0           0   \n",
              "\n",
              "   proto_ifmp  proto_igmp  proto_igp  proto_il  proto_ip  proto_ipcomp  \\\n",
              "0           0           0          0         0         0             0   \n",
              "1           0           0          0         0         0             0   \n",
              "2           0           0          0         0         0             0   \n",
              "3           0           0          0         0         0             0   \n",
              "4           0           0          0         0         0             0   \n",
              "\n",
              "   proto_ipcv  proto_ipip  proto_iplt  proto_ipnip  proto_ippc  proto_ipv6  \\\n",
              "0           0           0           0            0           0           0   \n",
              "1           0           0           0            0           0           0   \n",
              "2           0           0           0            0           0           0   \n",
              "3           0           0           0            0           0           0   \n",
              "4           0           0           0            0           0           0   \n",
              "\n",
              "   proto_ipv6-frag  proto_ipv6-no  proto_ipv6-opts  proto_ipv6-route  \\\n",
              "0                0              0                0                 0   \n",
              "1                0              0                0                 0   \n",
              "2                0              0                0                 0   \n",
              "3                0              0                0                 0   \n",
              "4                0              0                0                 0   \n",
              "\n",
              "   proto_ipx-n-ip  proto_irtp  proto_isis  proto_iso-ip  proto_iso-tp4  \\\n",
              "0               0           0           0             0              0   \n",
              "1               0           0           0             0              0   \n",
              "2               0           0           0             0              0   \n",
              "3               0           0           0             0              0   \n",
              "4               0           0           0             0              0   \n",
              "\n",
              "   proto_kryptolan  proto_l2tp  proto_larp  proto_leaf-1  proto_leaf-2  \\\n",
              "0                0           0           0             0             0   \n",
              "1                0           0           0             0             0   \n",
              "2                0           0           0             0             0   \n",
              "3                0           0           0             0             0   \n",
              "4                0           0           0             0             0   \n",
              "\n",
              "   proto_merit-inp  proto_mfe-nsp  proto_mhrp  proto_micp  proto_mobile  \\\n",
              "0                0              0           0           0             0   \n",
              "1                0              0           0           0             0   \n",
              "2                0              0           0           0             0   \n",
              "3                0              0           0           0             0   \n",
              "4                0              0           0           0             0   \n",
              "\n",
              "   proto_mtp  proto_mux  proto_narp  proto_netblt  proto_nsfnet-igp  \\\n",
              "0          0          0           0             0                 0   \n",
              "1          0          0           0             0                 0   \n",
              "2          0          0           0             0                 0   \n",
              "3          0          0           0             0                 0   \n",
              "4          0          0           0             0                 0   \n",
              "\n",
              "   proto_nvp  proto_ospf  proto_pgm  proto_pim  proto_pipe  proto_pnni  \\\n",
              "0          0           0          0          0           0           0   \n",
              "1          0           0          0          0           0           0   \n",
              "2          0           0          0          0           0           0   \n",
              "3          0           0          0          0           0           0   \n",
              "4          0           0          0          0           0           0   \n",
              "\n",
              "   proto_pri-enc  proto_prm  proto_ptp  proto_pup  proto_pvp  proto_qnx  \\\n",
              "0              0          0          0          0          0          0   \n",
              "1              0          0          0          0          0          0   \n",
              "2              0          0          0          0          0          0   \n",
              "3              0          0          0          0          0          0   \n",
              "4              0          0          0          0          0          0   \n",
              "\n",
              "   proto_rdp  proto_rsvp  proto_rtp  proto_rvd  proto_sat-expak  \\\n",
              "0          0           0          0          0                0   \n",
              "1          0           0          0          0                0   \n",
              "2          0           0          0          0                0   \n",
              "3          0           0          0          0                0   \n",
              "4          0           0          0          0                0   \n",
              "\n",
              "   proto_sat-mon  proto_sccopmce  proto_scps  proto_sctp  proto_sdrp  \\\n",
              "0              0               0           0           0           0   \n",
              "1              0               0           0           0           0   \n",
              "2              0               0           0           0           0   \n",
              "3              0               0           0           0           0   \n",
              "4              0               0           0           0           0   \n",
              "\n",
              "   proto_secure-vmtp  proto_sep  proto_skip  proto_sm  proto_smp  proto_snp  \\\n",
              "0                  0          0           0         0          0          0   \n",
              "1                  0          0           0         0          0          0   \n",
              "2                  0          0           0         0          0          0   \n",
              "3                  0          0           0         0          0          0   \n",
              "4                  0          0           0         0          0          0   \n",
              "\n",
              "   proto_sprite-rpc  proto_sps  proto_srp  proto_st2  proto_stp  proto_sun-nd  \\\n",
              "0                 0          0          0          0          0             0   \n",
              "1                 0          0          0          0          0             0   \n",
              "2                 0          0          0          0          0             0   \n",
              "3                 0          0          0          0          0             0   \n",
              "4                 0          0          0          0          0             0   \n",
              "\n",
              "   proto_swipe  proto_tcf  proto_tcp  proto_tlsp  proto_tp++  proto_trunk-1  \\\n",
              "0            0          0          0           0           0              0   \n",
              "1            0          0          0           0           0              0   \n",
              "2            0          0          0           0           0              0   \n",
              "3            0          0          0           0           0              0   \n",
              "4            0          0          0           0           0              0   \n",
              "\n",
              "   proto_trunk-2  proto_ttp  proto_udp  proto_unas  proto_uti  proto_vines  \\\n",
              "0              0          0          1           0          0            0   \n",
              "1              0          0          1           0          0            0   \n",
              "2              0          0          1           0          0            0   \n",
              "3              0          0          1           0          0            0   \n",
              "4              0          0          1           0          0            0   \n",
              "\n",
              "   proto_visa  proto_vmtp  proto_vrrp  proto_wb-expak  proto_wb-mon  \\\n",
              "0           0           0           0               0             0   \n",
              "1           0           0           0               0             0   \n",
              "2           0           0           0               0             0   \n",
              "3           0           0           0               0             0   \n",
              "4           0           0           0               0             0   \n",
              "\n",
              "   proto_wsn  proto_xnet  proto_xns-idp  proto_xtp  proto_zero  service_dhcp  \\\n",
              "0          0           0              0          0           0             0   \n",
              "1          0           0              0          0           0             0   \n",
              "2          0           0              0          0           0             0   \n",
              "3          0           0              0          0           0             0   \n",
              "4          0           0              0          0           0             0   \n",
              "\n",
              "   service_dns  service_ftp  service_ftp-data  service_http  service_irc  \\\n",
              "0            0            0                 0             0            0   \n",
              "1            0            0                 0             0            0   \n",
              "2            0            0                 0             0            0   \n",
              "3            0            0                 0             0            0   \n",
              "4            0            0                 0             0            0   \n",
              "\n",
              "   service_pop3  service_radius  service_smtp  service_snmp  service_ssh  \\\n",
              "0             0               0             0             0            0   \n",
              "1             0               0             0             0            0   \n",
              "2             0               0             0             0            0   \n",
              "3             0               0             0             0            0   \n",
              "4             0               0             0             0            0   \n",
              "\n",
              "   service_ssl  state_CLO  state_CON  state_ECO  state_FIN  state_INT  \\\n",
              "0            0          0          0          0          0          1   \n",
              "1            0          0          0          0          0          1   \n",
              "2            0          0          0          0          0          1   \n",
              "3            0          0          0          0          0          1   \n",
              "4            0          0          0          0          0          1   \n",
              "\n",
              "   state_PAR  state_REQ  state_RST  state_URN  state_no       dur  spkts  \\\n",
              "0          0          0          0          0         0  0.000011      2   \n",
              "1          0          0          0          0         0  0.000008      2   \n",
              "2          0          0          0          0         0  0.000005      2   \n",
              "3          0          0          0          0         0  0.000006      2   \n",
              "4          0          0          0          0         0  0.000010      2   \n",
              "\n",
              "   dpkts  sbytes  dbytes         rate  sttl  dttl        sload  dload  sloss  \\\n",
              "0      0     496       0   90909.0902   254     0  180363632.0    0.0      0   \n",
              "1      0    1762       0  125000.0003   254     0  881000000.0    0.0      0   \n",
              "2      0    1068       0  200000.0051   254     0  854400000.0    0.0      0   \n",
              "3      0     900       0  166666.6608   254     0  600000000.0    0.0      0   \n",
              "4      0    2126       0  100000.0025   254     0  850400000.0    0.0      0   \n",
              "\n",
              "   dloss  sinpkt  dinpkt  sjit  djit  swin  stcpb  dtcpb  dwin  tcprtt  \\\n",
              "0      0   0.011     0.0   0.0   0.0     0      0      0     0     0.0   \n",
              "1      0   0.008     0.0   0.0   0.0     0      0      0     0     0.0   \n",
              "2      0   0.005     0.0   0.0   0.0     0      0      0     0     0.0   \n",
              "3      0   0.006     0.0   0.0   0.0     0      0      0     0     0.0   \n",
              "4      0   0.010     0.0   0.0   0.0     0      0      0     0     0.0   \n",
              "\n",
              "   synack  ackdat  smean  dmean  trans_depth  response_body_len  ct_srv_src  \\\n",
              "0     0.0     0.0    248      0            0                  0           2   \n",
              "1     0.0     0.0    881      0            0                  0           2   \n",
              "2     0.0     0.0    534      0            0                  0           3   \n",
              "3     0.0     0.0    450      0            0                  0           3   \n",
              "4     0.0     0.0   1063      0            0                  0           3   \n",
              "\n",
              "   ct_state_ttl  ct_dst_ltm  ct_src_dport_ltm  ct_dst_sport_ltm  \\\n",
              "0             2           1                 1                 1   \n",
              "1             2           1                 1                 1   \n",
              "2             2           1                 1                 1   \n",
              "3             2           2                 2                 1   \n",
              "4             2           2                 2                 1   \n",
              "\n",
              "   ct_dst_src_ltm  is_ftp_login  ct_ftp_cmd  ct_flw_http_mthd  ct_src_ltm  \\\n",
              "0               2             0           0                 0           1   \n",
              "1               2             0           0                 0           1   \n",
              "2               3             0           0                 0           1   \n",
              "3               3             0           0                 0           2   \n",
              "4               3             0           0                 0           2   \n",
              "\n",
              "   ct_srv_dst  is_sm_ips_ports  \n",
              "0           2                0  \n",
              "1           2                0  \n",
              "2           3                0  \n",
              "3           3                0  \n",
              "4           3                0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X02B7Ohcbbt2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "55de2917-447f-4277-f46a-41f65fc8ef5c"
      },
      "source": [
        "tgt1.sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Analysis           2677\n",
              "Backdoor           2329\n",
              "DoS               16353\n",
              "Exploits          44525\n",
              "Fuzzers           24246\n",
              "Generic           58871\n",
              "Normal            93000\n",
              "Reconnaissance    13987\n",
              "Shellcode          1511\n",
              "Worms               174\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpFPsedJc3K8"
      },
      "source": [
        "ssc = MinMaxScaler()\n",
        "X = ssc.fit_transform(data_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVLVS2ogQyHw"
      },
      "source": [
        "att_cat = tgt0.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7RFl42uwwqR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "85a98cfd-fe28-437c-d606-e890134105f0"
      },
      "source": [
        "smt = SMOTE({'Worms':30000,'Shellcode':30000,'Backdoor':30000,'Analysis':30000,'Reconnaissance':30000,'DoS':30000 })\n",
        "X_smote,y_smote = smt.fit_sample(X,tgt0.values)\n",
        "y_smote = pd.get_dummies(y_smote).values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ry63_B8PMR9U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "76b22ddf-9974-475d-aedd-4a3a3b51f4f3"
      },
      "source": [
        "X_smote.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400642, 193)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irB5hsy9w3Qk"
      },
      "source": [
        "-------- \n",
        "\n",
        "### SMOTE and TSVD will also be considered"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9Ojl-L1QVIa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f8069712-32e7-4677-f13f-789c6949310e"
      },
      "source": [
        "tsver = tsvd(n_components=2)\n",
        "X_tsvd = tsver.fit_transform(data_new)\n",
        "np.sum(tsver.explained_variance_ratio_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9908312047204898"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkKPzw6QlICP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f5545fa4-4c78-4b96-f0ff-d9df4338a55b"
      },
      "source": [
        "tgt0.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Normal            93000\n",
              "Generic           58871\n",
              "Exploits          44525\n",
              "Fuzzers           24246\n",
              "DoS               16353\n",
              "Reconnaissance    13987\n",
              "Analysis           2677\n",
              "Backdoor           2329\n",
              "Shellcode          1511\n",
              "Worms               174\n",
              "Name: attack_cat, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guIHOe1uThrZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "0ae3de66-ccfb-406b-ed5f-0e75057fa8df"
      },
      "source": [
        "smt = SMOTE(sampling_strategy='not majority')\n",
        "X_smote_tsvd,y_smote_tsvd = smt.fit_sample(X_tsvd,tgt0.values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1Y8RwMxlpER"
      },
      "source": [
        "y_smote_tsvd = pd.get_dummies(y_smote_tsvd)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4RUyVQte12M"
      },
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(X_tsvd,tgt1,stratify=tgt1,shuffle=True,train_size=0.8,test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2ujOV9fkRRV"
      },
      "source": [
        "x_train1,x_test1,y_train1,y_test1 = train_test_split(X_smote,y_smote,stratify=y_smote,shuffle=True,train_size=0.9,test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMXn9Afps73_"
      },
      "source": [
        "x_train2,x_test2,y_train2,y_test2 = train_test_split(X,tgt1,stratify=tgt1,shuffle=True,train_size=0.9,test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Raoq402xEjE"
      },
      "source": [
        "x_train3,x_test3,y_train3,y_test3 = train_test_split(X_smote_tsvd,y_smote_tsvd,stratify=y_smote_tsvd,shuffle=True,train_size=0.9,test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78VDTD2M11bM"
      },
      "source": [
        "x_train4,x_test4,y_train4,y_test4 = train_test_split(X,tgt2,shuffle = True, stratify = tgt2, train_size = 0.8,test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1E7FM8ZHwbI"
      },
      "source": [
        "X_smote_train, y_smote_train = x_train1.reshape(x_train1.shape[0],x_train1.shape[1],1), y_train1.reshape(y_train1.shape[0],y_train1.shape[1],1)\n",
        "X_smote_test, y_smote_test = x_test1.reshape(x_test1.shape[0],x_test1.shape[1],1), y_test1.reshape(y_test1.shape[0],y_test1.shape[1],1)\n",
        "X_tsvd_train,X_tsvd_test = x_train.reshape(x_train.shape[0],x_train.shape[1],1),x_test.reshape(x_test.shape[0],x_test.shape[1],1)\n",
        "X_smotetsvd_train , X_smotetsvd_test = x_train3.reshape(x_train3.shape[0],x_train3.shape[1],1),x_test3.reshape(x_test3.shape[0],x_test3.shape[1],1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyOZhHOuIdYn"
      },
      "source": [
        "## First network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsuCTcXGXa6l"
      },
      "source": [
        "def identity_block(X, f, filters, stage, block):\n",
        "    \"\"\"\n",
        "    Implementation of the identity block as defined in Resnet 50 paper\n",
        "    Arguments:\n",
        "    X -- input tensor of shape (m, n_W_prev, 1)\n",
        "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
        "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
        "    stage -- integer, used to name the layers, depending on their position in the network\n",
        "    block -- string/character, used to name the layers, depending on their position in the network\n",
        "    Returns:\n",
        "    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
        "    \"\"\"\n",
        "\n",
        "    # defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "\n",
        "    # Save the input value. You'll need this later to add back to the main path.\n",
        "    X_shortcut = X\n",
        "\n",
        "    # First component of main path\n",
        "    X = Conv1D(filters=F1, kernel_size=1, strides=1, padding='valid', name=conv_name_base + '2a', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(name=bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Second component of main path\n",
        "    X = Conv1D(filters=F2, kernel_size=f, strides=1, padding='same', name=conv_name_base + '2b', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(name=bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path\n",
        "    X = Conv1D(filters=F3, kernel_size=1, strides=1, padding='valid', name=conv_name_base + '2c', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(name=bn_name_base + '2c')(X)\n",
        "\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation\n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "def convolutional_block(X, f, filters, stage, block, s=2):\n",
        "    \"\"\"\n",
        "    Implementation of the convolutional block as defined in Resnet 50 Paper\n",
        "    Arguments:\n",
        "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
        "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
        "    stage -- integer, used to name the layers, depending on their position in the network\n",
        "    block -- string/character, used to name the layers, depending on their position in the network\n",
        "    s -- Integer, specifying the stride to be used\n",
        "    Returns:\n",
        "    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
        "    \"\"\"\n",
        "\n",
        "    # defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "\n",
        "    # Save the input value\n",
        "    X_shortcut = X\n",
        "\n",
        "    # First component of main path\n",
        "    X = Conv1D(F1, 1, strides=s, name=conv_name_base + '2a', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization( name=bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Second component of main path\n",
        "    X = Conv1D(filters=F2, kernel_size=f, strides=1, padding='same', name=conv_name_base + '2b', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(name=bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path\n",
        "    X = Conv1D(filters=F3, kernel_size=1, strides=1, padding='valid', name=conv_name_base + '2c', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(name=bn_name_base + '2c')(X)\n",
        "\n",
        "    # Shortcut path\n",
        "    X_shortcut = Conv1D(F3, 1, strides=s, name=conv_name_base + '1', kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n",
        "    X_shortcut = BatchNormalization(name=bn_name_base + '1')(X_shortcut)\n",
        "\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f-RNQS8BLKd"
      },
      "source": [
        "def resnet_1d(input_shape,classes=10):\n",
        "  \n",
        "    \"\"\"\n",
        "    Returns:\n",
        "    model -- a Model() instance in Keras. The architecture is mutuated from \n",
        "    resnet concept.\n",
        "    \"\"\"\n",
        "    X_input = Input(input_shape)\n",
        "        \n",
        "    X = ZeroPadding1D(2)(X_input)\n",
        "   # Stage 1\n",
        "#   X = Conv1D(filters = 50,kernel_size = 3,strides=2,padding = 'valid',name = 'conv_layer_short_1',activity_regularizer=l2(l=0.02))(X)\n",
        "#   X = BatchNormalization(name='bn_conv1')(X)\n",
        "#   X = Activation('relu')(X)\n",
        "#   X = MaxPooling1D(3, strides=2)(X)\n",
        "## Following, there are different sections not used in the actual code stage.\n",
        "    X = Conv1D(filters = 20,kernel_size = 10,strides=2,padding = 'valid',name = 'conv_layer_short_2')(X)\n",
        "    X = BatchNormalization(name='bn_conv2')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling1D(3, strides=2)(X)\n",
        "   # Stage 2\n",
        "    X = convolutional_block(X, f=3, filters=[64, 64, 256], stage=2, block='a', s=2)\n",
        "    X = identity_block(X,1, [64, 64, 256], stage=2, block='b')\n",
        "    X = identity_block(X,1, [64, 64, 256], stage=2, block='c')\n",
        " \n",
        "  # Ultima modifica, nei blocchi qui sotto ed in stage 2, i 5 ed i 3 erano 1\n",
        " \n",
        "   # Stage 3\n",
        "    X = convolutional_block(X, f=3, filters=[128, 128, 512], stage=3, block='a', s=2)\n",
        "    X = identity_block(X, 1, [128, 128, 512], stage=3, block='b')\n",
        "    X = identity_block(X, 1, [128, 128, 512], stage=3, block='c')\n",
        "    X = identity_block(X, 1, [128, 128, 512], stage=3, block='d')\n",
        "  \n",
        "    # Stage 4\n",
        "    X = convolutional_block(X, f=3, filters=[256, 256, 1024], stage=4, block='a', s=2)\n",
        "    X = identity_block(X, 1, [256, 256, 1024], stage=4, block='b')\n",
        "    X = identity_block(X, 1, [256, 256, 1024], stage=4, block='c')\n",
        "    X = identity_block(X, 1, [256, 256, 1024], stage=4, block='d')\n",
        "    X = identity_block(X, 1, [256, 256, 1024], stage=4, block='e')\n",
        "    X = identity_block(X, 1, [256, 256, 1024], stage=4, block='f')\n",
        "  \n",
        "    # Stage 5\n",
        "    X = convolutional_block(X, f=3, filters=[512, 512, 2048], stage=5, block='a', s=2)\n",
        "    X = identity_block(X, 1, [512, 512, 2048], stage=5, block='b')\n",
        "    X = identity_block(X, 1, [512, 512, 2048], stage=5, block='c')\n",
        " \n",
        "    X = AveragePooling1D(1,name = 'AVG_POOL1')(X)\n",
        "   \n",
        "  \n",
        "    # output layer\n",
        "#    X = LSTM(50,return_sequences=True)(X)\n",
        "#    X = LSTM(classes)(X)\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(700, activation = 'relu',kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = Dropout(0.8)(X)\n",
        "    X = Dense(200, activation = 'relu',kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = Dropout(0.6)(X)\n",
        "    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "#    X = Conv1D(filters = 25,kernel_size = 1,strides=1,padding = 'valid',name = 'conv_layer_final1',activity_regularizer=l2(l=0.002))(X)\n",
        "#    X = Activation('relu')(X)\n",
        "#    X = Conv1D(filters = classes,kernel_size = 1,padding = 'same',name = 'conv_layer_final2',activity_regularizer=l2(l=0.002))(X)\n",
        "#    X = Activation('softmax')(X)\n",
        "#   Create model\n",
        "    model = Model(inputs=X_input, outputs=X, name='ResNet50')\n",
        "  \n",
        "    return model "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5nUnZK4aJQg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3d308c0f-ef37-4141-b38a-d393cbfdcddf"
      },
      "source": [
        "!pip install -q gputil\n",
        "!pip install -q psutil\n",
        "!pip install -q humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Gen RAM Free: 12.2 GB  | Proc size: 3.4 GB\n",
            "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total 16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYZ0qTCVLr5D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5e2394ae-4eb1-4fc8-adf3-2cb22d8891f5"
      },
      "source": [
        "X_smote_train.shape[1:3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(193, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3NKzXdJDEvP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "3889522b-a3de-475e-a864-bfde05bc0936"
      },
      "source": [
        "model_r = resnet_1d(input_shape=X_smote_train.shape[1:3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdO6Oq35QWgw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "297862f6-f930-4e82-d80b-1d46348cdc91"
      },
      "source": [
        "model_r.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"ResNet50\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 193, 1)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding1d_1 (ZeroPadding1D (None, 197, 1)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_layer_short_1 (Conv1D)     (None, 98, 50)       200         zero_padding1d_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn_conv1 (BatchNormalization)   (None, 98, 50)       200         conv_layer_short_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 98, 50)       0           bn_conv1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1D)  (None, 48, 50)       0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch2a (Conv1D)         (None, 24, 64)       3264        max_pooling1d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch2a (BatchNormalizati (None, 24, 64)       256         res2a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 24, 64)       0           bn2a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch2b (Conv1D)         (None, 24, 64)       12352       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch2b (BatchNormalizati (None, 24, 64)       256         res2a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 24, 64)       0           bn2a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch2c (Conv1D)         (None, 24, 256)      16640       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch1 (Conv1D)          (None, 24, 256)      13056       max_pooling1d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch2c (BatchNormalizati (None, 24, 256)      1024        res2a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch1 (BatchNormalizatio (None, 24, 256)      1024        res2a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 24, 256)      0           bn2a_branch2c[0][0]              \n",
            "                                                                 bn2a_branch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 24, 256)      0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch2a (Conv1D)         (None, 24, 64)       16448       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch2a (BatchNormalizati (None, 24, 64)       256         res2b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 24, 64)       0           bn2b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch2b (Conv1D)         (None, 24, 64)       4160        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch2b (BatchNormalizati (None, 24, 64)       256         res2b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 24, 64)       0           bn2b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch2c (Conv1D)         (None, 24, 256)      16640       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch2c (BatchNormalizati (None, 24, 256)      1024        res2b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 24, 256)      0           bn2b_branch2c[0][0]              \n",
            "                                                                 activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 24, 256)      0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch2a (Conv1D)         (None, 24, 64)       16448       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch2a (BatchNormalizati (None, 24, 64)       256         res2c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 24, 64)       0           bn2c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch2b (Conv1D)         (None, 24, 64)       4160        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch2b (BatchNormalizati (None, 24, 64)       256         res2c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 24, 64)       0           bn2c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch2c (Conv1D)         (None, 24, 256)      16640       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch2c (BatchNormalizati (None, 24, 256)      1024        res2c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 24, 256)      0           bn2c_branch2c[0][0]              \n",
            "                                                                 activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 24, 256)      0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch2a (Conv1D)         (None, 12, 128)      32896       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch2a (BatchNormalizati (None, 12, 128)      512         res3a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 12, 128)      0           bn3a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch2b (Conv1D)         (None, 12, 128)      49280       activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch2b (BatchNormalizati (None, 12, 128)      512         res3a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 12, 128)      0           bn3a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch2c (Conv1D)         (None, 12, 512)      66048       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch1 (Conv1D)          (None, 12, 512)      131584      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch2c (BatchNormalizati (None, 12, 512)      2048        res3a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch1 (BatchNormalizatio (None, 12, 512)      2048        res3a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 12, 512)      0           bn3a_branch2c[0][0]              \n",
            "                                                                 bn3a_branch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 12, 512)      0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch2a (Conv1D)         (None, 12, 128)      65664       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch2a (BatchNormalizati (None, 12, 128)      512         res3b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 12, 128)      0           bn3b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch2b (Conv1D)         (None, 12, 128)      16512       activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch2b (BatchNormalizati (None, 12, 128)      512         res3b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 12, 128)      0           bn3b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch2c (Conv1D)         (None, 12, 512)      66048       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch2c (BatchNormalizati (None, 12, 512)      2048        res3b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 12, 512)      0           bn3b_branch2c[0][0]              \n",
            "                                                                 activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 12, 512)      0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch2a (Conv1D)         (None, 12, 128)      65664       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch2a (BatchNormalizati (None, 12, 128)      512         res3c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 12, 128)      0           bn3c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch2b (Conv1D)         (None, 12, 128)      16512       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch2b (BatchNormalizati (None, 12, 128)      512         res3c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 12, 128)      0           bn3c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch2c (Conv1D)         (None, 12, 512)      66048       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch2c (BatchNormalizati (None, 12, 512)      2048        res3c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 12, 512)      0           bn3c_branch2c[0][0]              \n",
            "                                                                 activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 12, 512)      0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res3d_branch2a (Conv1D)         (None, 12, 128)      65664       activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3d_branch2a (BatchNormalizati (None, 12, 128)      512         res3d_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 12, 128)      0           bn3d_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3d_branch2b (Conv1D)         (None, 12, 128)      16512       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3d_branch2b (BatchNormalizati (None, 12, 128)      512         res3d_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 12, 128)      0           bn3d_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3d_branch2c (Conv1D)         (None, 12, 512)      66048       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3d_branch2c (BatchNormalizati (None, 12, 512)      2048        res3d_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 12, 512)      0           bn3d_branch2c[0][0]              \n",
            "                                                                 activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 12, 512)      0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch2a (Conv1D)         (None, 6, 256)       131328      activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch2a (BatchNormalizati (None, 6, 256)       1024        res4a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 6, 256)       0           bn4a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch2b (Conv1D)         (None, 6, 256)       196864      activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch2b (BatchNormalizati (None, 6, 256)       1024        res4a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 6, 256)       0           bn4a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch2c (Conv1D)         (None, 6, 1024)      263168      activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch1 (Conv1D)          (None, 6, 1024)      525312      activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch2c (BatchNormalizati (None, 6, 1024)      4096        res4a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch1 (BatchNormalizatio (None, 6, 1024)      4096        res4a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 6, 1024)      0           bn4a_branch2c[0][0]              \n",
            "                                                                 bn4a_branch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 6, 1024)      0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch2a (Conv1D)         (None, 6, 256)       262400      activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch2a (BatchNormalizati (None, 6, 256)       1024        res4b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 6, 256)       0           bn4b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch2b (Conv1D)         (None, 6, 256)       65792       activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch2b (BatchNormalizati (None, 6, 256)       1024        res4b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 6, 256)       0           bn4b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch2c (Conv1D)         (None, 6, 1024)      263168      activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch2c (BatchNormalizati (None, 6, 1024)      4096        res4b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 6, 1024)      0           bn4b_branch2c[0][0]              \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 6, 1024)      0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch2a (Conv1D)         (None, 6, 256)       262400      activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch2a (BatchNormalizati (None, 6, 256)       1024        res4c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 6, 256)       0           bn4c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch2b (Conv1D)         (None, 6, 256)       65792       activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch2b (BatchNormalizati (None, 6, 256)       1024        res4c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 6, 256)       0           bn4c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch2c (Conv1D)         (None, 6, 1024)      263168      activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch2c (BatchNormalizati (None, 6, 1024)      4096        res4c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 6, 1024)      0           bn4c_branch2c[0][0]              \n",
            "                                                                 activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 6, 1024)      0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4d_branch2a (Conv1D)         (None, 6, 256)       262400      activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4d_branch2a (BatchNormalizati (None, 6, 256)       1024        res4d_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 6, 256)       0           bn4d_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4d_branch2b (Conv1D)         (None, 6, 256)       65792       activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4d_branch2b (BatchNormalizati (None, 6, 256)       1024        res4d_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 6, 256)       0           bn4d_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4d_branch2c (Conv1D)         (None, 6, 1024)      263168      activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4d_branch2c (BatchNormalizati (None, 6, 1024)      4096        res4d_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 6, 1024)      0           bn4d_branch2c[0][0]              \n",
            "                                                                 activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 6, 1024)      0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4e_branch2a (Conv1D)         (None, 6, 256)       262400      activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4e_branch2a (BatchNormalizati (None, 6, 256)       1024        res4e_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 6, 256)       0           bn4e_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4e_branch2b (Conv1D)         (None, 6, 256)       65792       activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4e_branch2b (BatchNormalizati (None, 6, 256)       1024        res4e_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 6, 256)       0           bn4e_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4e_branch2c (Conv1D)         (None, 6, 1024)      263168      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4e_branch2c (BatchNormalizati (None, 6, 1024)      4096        res4e_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 6, 1024)      0           bn4e_branch2c[0][0]              \n",
            "                                                                 activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 6, 1024)      0           add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4f_branch2a (Conv1D)         (None, 6, 256)       262400      activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4f_branch2a (BatchNormalizati (None, 6, 256)       1024        res4f_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 6, 256)       0           bn4f_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4f_branch2b (Conv1D)         (None, 6, 256)       65792       activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4f_branch2b (BatchNormalizati (None, 6, 256)       1024        res4f_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 6, 256)       0           bn4f_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4f_branch2c (Conv1D)         (None, 6, 1024)      263168      activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4f_branch2c (BatchNormalizati (None, 6, 1024)      4096        res4f_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 6, 1024)      0           bn4f_branch2c[0][0]              \n",
            "                                                                 activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 6, 1024)      0           add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch2a (Conv1D)         (None, 3, 512)       524800      activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch2a (BatchNormalizati (None, 3, 512)       2048        res5a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 3, 512)       0           bn5a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch2b (Conv1D)         (None, 3, 512)       786944      activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch2b (BatchNormalizati (None, 3, 512)       2048        res5a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 3, 512)       0           bn5a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch2c (Conv1D)         (None, 3, 2048)      1050624     activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch1 (Conv1D)          (None, 3, 2048)      2099200     activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch2c (BatchNormalizati (None, 3, 2048)      8192        res5a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch1 (BatchNormalizatio (None, 3, 2048)      8192        res5a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 3, 2048)      0           bn5a_branch2c[0][0]              \n",
            "                                                                 bn5a_branch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 3, 2048)      0           add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch2a (Conv1D)         (None, 3, 512)       1049088     activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch2a (BatchNormalizati (None, 3, 512)       2048        res5b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 3, 512)       0           bn5b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch2b (Conv1D)         (None, 3, 512)       262656      activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch2b (BatchNormalizati (None, 3, 512)       2048        res5b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 3, 512)       0           bn5b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch2c (Conv1D)         (None, 3, 2048)      1050624     activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch2c (BatchNormalizati (None, 3, 2048)      8192        res5b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 3, 2048)      0           bn5b_branch2c[0][0]              \n",
            "                                                                 activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 3, 2048)      0           add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch2a (Conv1D)         (None, 3, 512)       1049088     activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch2a (BatchNormalizati (None, 3, 512)       2048        res5c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 3, 512)       0           bn5c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch2b (Conv1D)         (None, 3, 512)       262656      activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch2b (BatchNormalizati (None, 3, 512)       2048        res5c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 3, 512)       0           bn5c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch2c (Conv1D)         (None, 3, 2048)      1050624     activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch2c (BatchNormalizati (None, 3, 2048)      8192        res5c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 3, 2048)      0           bn5c_branch2c[0][0]              \n",
            "                                                                 activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 3, 2048)      0           add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "AVG_POOL1 (AveragePooling1D)    (None, 3, 2048)      0           activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 3, 100)       859600      AVG_POOL1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3, 200)       20200       lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 3, 200)       0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "fc10 (Dense)                    (None, 3, 10)        2010        dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 15,092,258\n",
            "Trainable params: 15,039,166\n",
            "Non-trainable params: 53,092\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dB5EQrIGFKD2"
      },
      "source": [
        "# Model R compiling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOgMZO4B2QQb"
      },
      "source": [
        "history=History()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DraR1ANZZ9YL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6f43a416-b0b6-4c3b-b02f-504be20394ec"
      },
      "source": [
        "model_r.compile(optimizer=Adam(lr=0.3,decay=0.0002,beta_1=0.95,beta_2=0.9995),loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXcuMAFtax9N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b11d9ade-d1aa-4713-df7a-cc81356a3939"
      },
      "source": [
        "printm()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 12.2 GB  | Proc size: 3.4 GB\n",
            "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total 16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_R2r0hOFGQ9"
      },
      "source": [
        "## Alternative models\n",
        "\n",
        "The second model conv/recurrent model.\n",
        "[This paper](https://www.researchgate.net/publication/312170608_Shallow_and_Deep_Networks_Intrusion_Detection_System_A_Taxonomy_and_Survey) and [this paper](https://www.slideshare.net/vinaykumarR2/icacci-presentationintrusion) show the relevant research.\n",
        "\n",
        "LSTM works in the following way:\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1000/1*goJVQs-p9kgLODFNyhl9zA.gif)\n",
        "\n",
        "### Other implementations:\n",
        "\n",
        "[CNN LSTM](https://machinelearningmastery.com/cnn-long-short-term-memory-networks/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yuGWkt7PAfc"
      },
      "source": [
        "def model_creation(attack,dropout_rate=[0.4,0.2,0.4]):\n",
        "  model = Sequential()\n",
        "#  model.add(Dense(1500,activation='relu',kernel_initializer='glorot_normal',input_shape=(attack.shape[1],))) # Try with more neurons per unit and with more dropouts. Now rate=0.2 and 0.1 tomorrow rate = 0.4 and 0.5 and more layers. Prolly conv layer necessary.\n",
        "#  model.add(BatchNormalization())\n",
        "#  model.add(Dropout(rate = 0.4))\n",
        "  model.add(Dense(2000,activation = 'relu',kernel_initializer='glorot_normal'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(rate = 0.6))\n",
        "  #model.add(Dense(800,activation = 'relu',kernel_initializer='glorot_normal'))\n",
        "#  model.add(Dense(700,activation = 'relu',kernel_initializer='glorot_normal'))\n",
        "#  model.add(Dropout(rate=dropout_rate[0]))\n",
        "#  model.add(BatchNormalization())\n",
        "  #model.add(Dense(800,activation = 'relu',kernel_initializer='glorot_normal'))\n",
        "  model.add(Dropout(rate=dropout_rate[1]))\n",
        "  model.add(Dense(1500,activation = 'relu',kernel_initializer='glorot_normal'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(rate = 0.4))  \n",
        "  model.add(Dense(500,activation = 'relu',kernel_initializer='glorot_normal'))\n",
        "  model.add(BatchNormalization()) #da valutare se tenerlo o meno.\n",
        "  model.add(Dropout(rate = 0.4))\n",
        "  model.add(Dense(350, activation = 'relu',kernel_initializer = 'glorot_normal'))\n",
        "  model.add(Dropout(rate=dropout_rate[2]))\n",
        "  model.add(Dense(100,activation = 'relu',kernel_initializer='glorot_normal'))\n",
        "  model.add(Dropout(rate = 0.4))\n",
        "  model.add(Dense(35,activation = 'relu',kernel_initializer = 'glorot_normal'))\n",
        "  model.add(Dense(10,activation='softmax'))\n",
        "  history=History()\n",
        "  return model\n",
        "\n",
        "\n",
        "def model2(input_shape,conv=True,n_classes = 10):\n",
        "  \n",
        "  X_input = Input(input_shape,name = 'input_layer')\n",
        "  \n",
        "  if conv == True:\n",
        "    \n",
        "    \n",
        "    X_short = Conv1D(filters = 150,kernel_size = 7,strides=5,padding = 'valid',\n",
        "                     name = 'conv_layer_short',kernel_initializer = 'glorot_normal')(X_input)\n",
        "    X_short = BatchNormalization(momentum=0.995,scale=True,axis = 1, \n",
        "                                 name = 'Batch_Norm_short')(X_short) \n",
        "    X = Conv1D(filters = 25, kernel_size = 7,strides=1,padding = 'same',\n",
        "               name = 'fourth_layer',kernel_initializer = 'glorot_normal')(X_input)\n",
        "    X = BatchNormalization(axis = 1, name = 'Batch_Norm5')(X)    \n",
        "    X = Activation('relu')(X)\n",
        "    X = Conv1D(filters = 50, kernel_size = 5,strides=1,padding = 'same',\n",
        "               name = 'third_layer',kernel_initializer = 'glorot_normal')(X)\n",
        "    X = BatchNormalization(axis = 1,name ='Batch_Norm4')(X)\n",
        "    X = Activation('relu')(X)    \n",
        "    X = Conv1D(filters = 100, kernel_size = 5,strides=1,padding = 'same',\n",
        "               name = 'second_layer',kernel_initializer = 'glorot_normal')(X)\n",
        "    X = BatchNormalization(axis = 1,name ='Batch_Norm3')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling1D(pool_size=5)(X)        \n",
        "    X = Conv1D(filters = 125, kernel_size = 7,strides=1,padding = 'same',\n",
        "               name = 'second_layer_1',kernel_initializer = 'glorot_normal')(X)\n",
        "    X = BatchNormalization(axis = 1,name ='Batch_Norm2')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = Conv1D(filters = 150, kernel_size = 7,strides=1,padding = 'same',\n",
        "               name = 'first_layer',kernel_initializer = 'glorot_normal')(X)\n",
        "    X = BatchNormalization(axis = 1,name ='Batch_Norm1')(X)  \n",
        "    X = Activation('relu')(X)\n",
        "    X = Conv1D(filters = 150, kernel_size = 7,strides=1,padding = 'same',\n",
        "               name = 'second_layer_2',kernel_initializer = 'glorot_normal')(X)\n",
        "    \n",
        "#    X = Activation('relu')(X)\n",
        "#    X = MaxPooling1D(pool_size=7)(X)    \n",
        "#   X = MaxPooling1D(pool_size=2,padding='same')(X)    \n",
        "#    X = Activation('relu')(X)\n",
        "    \n",
        "    X = BatchNormalization(axis = 1,name ='Batch_Norm6')(X)\n",
        "    X = Add(name='Add_Layer')([X,X_short])\n",
        "    \n",
        "    X = Activation('relu')(X)\n",
        "#    X = LSTM(100, return_sequences=True)(X)\n",
        "    \n",
        "    X = Conv1D(filters = 300, kernel_size = 3,strides=1,padding = 'same',\n",
        "               name = 'final_layer1',kernel_initializer = 'glorot_normal')(X)\n",
        "    X = BatchNormalization(axis = 1,name ='Batch_Norm7')(X)  \n",
        "    X = Activation('relu')(X)\n",
        "    X = Conv1D(filters = 300, kernel_size = 3,strides=1,padding = 'same',\n",
        "               name = 'final_layer2',kernel_initializer = 'glorot_normal')(X)\n",
        "    X = BatchNormalization(axis = 1,name ='Batch_Norm8')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling1D(pool_size = 3, padding = 'same',name = 'pool_2')(X)\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(400,activation = 'relu',name='FC1',\n",
        "              kernel_initializer = 'glorot_normal')(X)\n",
        "    X = Dropout(rate=0.6)(X)\n",
        "    X = Dense(200,activation = 'relu',name='FC2',\n",
        "              kernel_initializer = 'glorot_normal')(X)\n",
        "    X = Dropout(rate=0.4)(X)\n",
        "    X = Dense(n_classes,activation = 'softmax',name='output_layer',kernel_initializer = 'glorot_normal')(X)\n",
        "    model = Model(inputs=X_input, outputs = X,name = 'Convnet')\n",
        "  \n",
        "  else:\n",
        "    \n",
        "#    X = Dense(1000,activation='relu',kernel_initializer='glorot_normal')(X_input) # Try with more neurons per unit and with more dropouts. Now rate=0.2 and 0.1 tomorrow rate = 0.4 and 0.5 and more layers. Prolly conv layer necessary.\n",
        "#    X = BatchNormalization()(X)\n",
        "#    X = Dropout(rate = 0.4)(X)\n",
        "#    X = Embedding(1000,128)(X_input)\n",
        "#    X = BatchNormalization()(X)\n",
        "#    X = Dropout(rate = 0.2)(X)\n",
        "#    X = Dense(1500,activation = 'relu',kernel_initializer='glorot_normal')(X)\n",
        "#    X = BatchNormalization()(X)\n",
        "#    X = Dropout(rate = 0.4)(X)\n",
        "#    X = Reshape(input_shape=(1500,),target_shape=(1500,1))(X)\n",
        "#!    X = Conv1D(filters = 64, kernel_size = 3,strides=2,padding = 'valid',kernel_initializer='glorot_normal',name = 'second_layer_1a')(X)\n",
        "#!    X = BatchNormalization(axis = 1,name ='Batch_Norm1')(X)  \n",
        "#!    X = Activation('relu')(X)\n",
        "#!    X = Conv1D(filters = 64, kernel_size = 3,strides=1,padding = 'valid',kernel_initializer='glorot_normal',name = 'second_layer_1b')(X)\n",
        "#!    X = BatchNormalization(axis = 1,name ='Batch_Norm2')(X)  \n",
        "#!    X = Activation('relu')(X)\n",
        "#    X = MaxPooling1D(pool_size=20,padding='same')(X)\n",
        "    X = Conv1D(filters = 25, kernel_size = 7,strides=1,padding = 'same',kernel_initializer='glorot_normal',name = 'second_layer_2a')(X_input)\n",
        "    X = BatchNormalization(axis = 1,name ='Batch_Norm3')(X)  \n",
        "    X = Activation('relu')(X)\n",
        "    X = Conv1D(filters = 50, kernel_size = 7,strides=1,padding = 'valid',kernel_initializer='glorot_normal',name = 'second_layer_2b')(X)\n",
        "    X = BatchNormalization(axis = 1,name ='Batch_Norm4')(X)  \n",
        "    X = Activation('relu')(X)\n",
        " #   X = MaxPooling1D(pool_size=2,padding='same')(X)\n",
        "    X = Conv1D(filters = 100, kernel_size = 9,strides=1,padding = 'valid',kernel_initializer='glorot_normal',name = 'second_layer_3')(X)\n",
        "    X = BatchNormalization(axis = 1,name ='Batch_Norm5')(X)  \n",
        "    X = Activation('relu')(X)\n",
        "    X = Conv1D(filters = 200, kernel_size = 9,strides=1,padding = 'same',kernel_initializer='glorot_normal',name = 'second_layer_4')(X)\n",
        "    X = BatchNormalization(axis = 1,name ='Batch_Norm51')(X)  \n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling1D(pool_size=5,padding='same')(X)\n",
        "    X = Flatten()(X)\n",
        "#    X = LSTM(200,return_sequences=True)(X)\n",
        "#    X = LSTM(100)(X)\n",
        "#    X = Dense(300,activation = 'relu',kernel_initializer='glorot_normal')(X)\n",
        "#    X = BatchNormalization()(X)\n",
        "#    X = Dropout(rate=0.4)(X)\n",
        "#!    X = Dense(750,activation = 'relu',kernel_initializer='glorot_normal')(X)\n",
        "#!    X = BatchNormalization()(X)\n",
        "#!    X = Dropout(rate=0.5)(X)\n",
        "#!    X = Dense(300,activation = 'relu',kernel_initializer='glorot_normal')(X)\n",
        "#!    X = BatchNormalization()(X) #da valutare se tenerlo o meno.\n",
        "#    X = Dense(750, activation = 'relu',kernel_initializer = 'glorot_normal')(X)\n",
        "#    X = Dropout(rate = 0.2)(X)\n",
        "#!    X = Reshape(input_shape=(300,),target_shape=(300,1))(X)  \n",
        "#!    X = LSTM(n_classes,activation='softmax')(X)\n",
        "    X = Dense(150,activation = 'relu',kernel_initializer='glorot_normal')(X)\n",
        "#    X = Dropout(rate = 0.4)(X)\n",
        "    X = Dense(50,activation = 'relu',kernel_initializer = 'glorot_normal')(X)#    X = Dropout(rate = 0.4)(X)\n",
        "    X = Dense(n_classes,activation='softmax')(X)\n",
        "    model = Model(inputs=X_input, outputs = X,name = 'FullyConnected')\n",
        "  return model  \n",
        "\n",
        " #Last: Without majority class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2p28HMFZnxn"
      },
      "source": [
        "def modded_resnet(input_shape,classes=10):\n",
        "  X_input = Input(input_shape)\n",
        "      \n",
        "  X = ZeroPadding1D(2)(X_input)\n",
        "  \n",
        "  X = Conv1D(filters = 50,kernel_size = 3,strides=2,padding = 'valid',name = 'conv_layer_short_1',activity_regularizer=l2(l=0.02))(X)\n",
        "  X = BatchNormalization(name='bn_conv1')(X)\n",
        "  X = Activation('relu')(X)\n",
        "  X = MaxPooling1D(3, strides=2)(X)\n",
        "  \n",
        "  X = convolutional_block(X, f=7, filters=[25, 25, 100], stage=2, block='a', s=3)\n",
        "  X = identity_block(X,3, [25, 25, 100], stage=2, block='b')\n",
        "  X = identity_block(X,3, [25, 25, 100], stage=2, block='c')\n",
        "  X = identity_block(X,3, [25, 25, 100], stage=2, block='d')\n",
        "  \n",
        "  X = convolutional_block(X, f=7, filters=[50, 50, 200], stage=3, block='a', s=3)\n",
        "  X = identity_block(X,3, [50, 50, 200], stage=3, block='b')\n",
        "  X = identity_block(X,3, [50, 50, 200], stage=3, block='c')\n",
        "  X = identity_block(X,3, [50, 50, 200], stage=3, block='d')\n",
        "#  X = LSTM(400,return_sequences=True)(X)\n",
        "  X = convolutional_block(X, f=7, filters=[100, 100, 400], stage=4, block='a', s=3)\n",
        "  X = identity_block(X,3, [100, 100, 400], stage=4, block='b')\n",
        "  X = identity_block(X,3, [100, 100, 400], stage=4, block='c')\n",
        "  X = identity_block(X,3, [100, 100, 400], stage=4, block='d')\n",
        "  X = identity_block(X,3, [100, 100, 400], stage=4, block='e')\n",
        "\n",
        "  X = convolutional_block(X, f=7, filters=[200, 200, 800], stage=5, block='a', s=3)\n",
        "  X = identity_block(X,3, [200, 200, 800], stage=5, block='b')\n",
        "  X = identity_block(X,3, [200, 200, 800], stage=5, block='c')\n",
        "  X = identity_block(X,3, [200, 200, 800], stage=5, block='d')\n",
        "  X = identity_block(X,3, [200, 200, 800], stage=5, block='e')\n",
        "#  X = MaxPooling1D(2,name = 'AVG_POOL1')(X)\n",
        "  X = Flatten()(X)\n",
        "  X = Dense(600, activation = 'relu',kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "  X = Dropout(0.6)(X)\n",
        "  X = Dense(100, activation = 'relu',kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "  X = Dropout(0.4)(X)\n",
        "  X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "  \n",
        "  model = Model(inputs=X_input, outputs=X, name='ResNet50')\n",
        "  \n",
        "  return model "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZmlS5BN_yXA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2a764c5a-e417-4983-ff8b-e6f717437686"
      },
      "source": [
        "model = model2(X_smote_train.shape[1:3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMOzLo40-PxY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0fdccfc4-6ca7-48ab-de5f-3df5d0626181"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Convnet\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_layer (InputLayer)        (None, 193, 1)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "fourth_layer (Conv1D)           (None, 193, 25)      200         input_layer[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Batch_Norm5 (BatchNormalization (None, 193, 25)      772         fourth_layer[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 193, 25)      0           Batch_Norm5[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "third_layer (Conv1D)            (None, 193, 50)      6300        activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Batch_Norm4 (BatchNormalization (None, 193, 50)      772         third_layer[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 193, 50)      0           Batch_Norm4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "second_layer (Conv1D)           (None, 193, 100)     25100       activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Batch_Norm3 (BatchNormalization (None, 193, 100)     772         second_layer[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 193, 100)     0           Batch_Norm3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1D)  (None, 38, 100)      0           activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "second_layer_1 (Conv1D)         (None, 38, 125)      87625       max_pooling1d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "Batch_Norm2 (BatchNormalization (None, 38, 125)      152         second_layer_1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 38, 125)      0           Batch_Norm2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "first_layer (Conv1D)            (None, 38, 150)      131400      activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Batch_Norm1 (BatchNormalization (None, 38, 150)      152         first_layer[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 38, 150)      0           Batch_Norm1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "second_layer_2 (Conv1D)         (None, 38, 150)      157650      activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_layer_short (Conv1D)       (None, 38, 150)      1200        input_layer[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Batch_Norm6 (BatchNormalization (None, 38, 150)      152         second_layer_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Batch_Norm_short (BatchNormaliz (None, 38, 150)      152         conv_layer_short[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "Add_Layer (Add)                 (None, 38, 150)      0           Batch_Norm6[0][0]                \n",
            "                                                                 Batch_Norm_short[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 38, 150)      0           Add_Layer[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "final_layer1 (Conv1D)           (None, 38, 300)      135300      activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Batch_Norm7 (BatchNormalization (None, 38, 300)      152         final_layer1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 38, 300)      0           Batch_Norm7[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "final_layer2 (Conv1D)           (None, 38, 300)      270300      activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Batch_Norm8 (BatchNormalization (None, 38, 300)      152         final_layer2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 38, 300)      0           Batch_Norm8[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "pool_2 (MaxPooling1D)           (None, 13, 300)      0           activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 3900)         0           pool_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "FC1 (Dense)                     (None, 400)          1560400     flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 400)          0           FC1[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "FC2 (Dense)                     (None, 200)          80200       dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 200)          0           FC2[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "output_layer (Dense)            (None, 10)           2010        dropout_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 2,460,913\n",
            "Trainable params: 2,459,299\n",
            "Non-trainable params: 1,614\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJt4RAKQZVTK"
      },
      "source": [
        "About batches:\n",
        "\n",
        "[click here](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eU-hrv9r5E4q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ec655cbb-9082-46d2-d9cd-4df6760fc653"
      },
      "source": [
        "history=History()\n",
        "model.compile(optimizer=Adam(lr=0.001,decay=1e-8,beta_1=0.9,beta_2=0.9995),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.fit(X_smote_train,y_train1,validation_split=0.1,epochs=1000,callbacks=[history],batch_size=2**11) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 324519 samples, validate on 36058 samples\n",
            "Epoch 1/1000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "324519/324519 [==============================] - 54s 167us/step - loss: 1.2601 - acc: 0.5309 - val_loss: 1.0448 - val_acc: 0.5999\n",
            "Epoch 2/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.8978 - acc: 0.6268 - val_loss: 0.9439 - val_acc: 0.5966\n",
            "Epoch 3/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.8282 - acc: 0.6556 - val_loss: 0.7458 - val_acc: 0.7067\n",
            "Epoch 4/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.7823 - acc: 0.6842 - val_loss: 0.7722 - val_acc: 0.6876\n",
            "Epoch 5/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.7593 - acc: 0.6933 - val_loss: 0.7472 - val_acc: 0.6992\n",
            "Epoch 6/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.7307 - acc: 0.7061 - val_loss: 0.7558 - val_acc: 0.7075\n",
            "Epoch 7/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.7122 - acc: 0.7126 - val_loss: 0.6909 - val_acc: 0.7233\n",
            "Epoch 8/1000\n",
            "324519/324519 [==============================] - 35s 106us/step - loss: 0.7139 - acc: 0.7123 - val_loss: 0.7156 - val_acc: 0.7207\n",
            "Epoch 9/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.6997 - acc: 0.7162 - val_loss: 0.7270 - val_acc: 0.7130\n",
            "Epoch 10/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.6767 - acc: 0.7244 - val_loss: 0.6531 - val_acc: 0.7400\n",
            "Epoch 11/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.6724 - acc: 0.7263 - val_loss: 0.7269 - val_acc: 0.7185\n",
            "Epoch 12/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.6637 - acc: 0.7290 - val_loss: 0.6622 - val_acc: 0.7346\n",
            "Epoch 13/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.6709 - acc: 0.7257 - val_loss: 0.6746 - val_acc: 0.7347\n",
            "Epoch 14/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.6519 - acc: 0.7325 - val_loss: 0.6385 - val_acc: 0.7425\n",
            "Epoch 15/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.6582 - acc: 0.7297 - val_loss: 1.0251 - val_acc: 0.6204\n",
            "Epoch 16/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.6605 - acc: 0.7292 - val_loss: 0.7169 - val_acc: 0.7176\n",
            "Epoch 17/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.6515 - acc: 0.7328 - val_loss: 0.6476 - val_acc: 0.7411\n",
            "Epoch 18/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.6678 - acc: 0.7264 - val_loss: 0.6582 - val_acc: 0.7371\n",
            "Epoch 19/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.6427 - acc: 0.7364 - val_loss: 0.6278 - val_acc: 0.7423\n",
            "Epoch 20/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.6359 - acc: 0.7371 - val_loss: 0.6445 - val_acc: 0.7432\n",
            "Epoch 21/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.6421 - acc: 0.7357 - val_loss: 0.6346 - val_acc: 0.7417\n",
            "Epoch 22/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.6242 - acc: 0.7423 - val_loss: 0.6226 - val_acc: 0.7450\n",
            "Epoch 23/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.6255 - acc: 0.7418 - val_loss: 0.6405 - val_acc: 0.7422\n",
            "Epoch 24/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.6478 - acc: 0.7326 - val_loss: 0.6463 - val_acc: 0.7406\n",
            "Epoch 25/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.6267 - acc: 0.7408 - val_loss: 0.6471 - val_acc: 0.7399\n",
            "Epoch 26/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.6149 - acc: 0.7447 - val_loss: 0.6147 - val_acc: 0.7479\n",
            "Epoch 27/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.6126 - acc: 0.7463 - val_loss: 1.2106 - val_acc: 0.5928\n",
            "Epoch 28/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.6114 - acc: 0.7465 - val_loss: 0.6222 - val_acc: 0.7446\n",
            "Epoch 29/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.6006 - acc: 0.7501 - val_loss: 0.6649 - val_acc: 0.7335\n",
            "Epoch 30/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.5974 - acc: 0.7508 - val_loss: 0.5989 - val_acc: 0.7556\n",
            "Epoch 31/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.6059 - acc: 0.7480 - val_loss: 0.6295 - val_acc: 0.7406\n",
            "Epoch 32/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.5952 - acc: 0.7520 - val_loss: 0.6194 - val_acc: 0.7501\n",
            "Epoch 33/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.5903 - acc: 0.7533 - val_loss: 0.5974 - val_acc: 0.7534\n",
            "Epoch 34/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.5949 - acc: 0.7518 - val_loss: 0.6127 - val_acc: 0.7493\n",
            "Epoch 35/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.5867 - acc: 0.7553 - val_loss: 0.5875 - val_acc: 0.7536\n",
            "Epoch 36/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.6015 - acc: 0.7500 - val_loss: 0.5844 - val_acc: 0.7576\n",
            "Epoch 37/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.5935 - acc: 0.7524 - val_loss: 0.5895 - val_acc: 0.7564\n",
            "Epoch 38/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.5803 - acc: 0.7583 - val_loss: 0.5830 - val_acc: 0.7583\n",
            "Epoch 39/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.5795 - acc: 0.7582 - val_loss: 0.5940 - val_acc: 0.7599\n",
            "Epoch 40/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.5685 - acc: 0.7611 - val_loss: 0.6801 - val_acc: 0.7370\n",
            "Epoch 41/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.5708 - acc: 0.7614 - val_loss: 0.5815 - val_acc: 0.7578\n",
            "Epoch 42/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.5739 - acc: 0.7592 - val_loss: 0.5917 - val_acc: 0.7587\n",
            "Epoch 43/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.5656 - acc: 0.7622 - val_loss: 0.6333 - val_acc: 0.7401\n",
            "Epoch 44/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.5558 - acc: 0.7658 - val_loss: 0.5547 - val_acc: 0.7683\n",
            "Epoch 45/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.5577 - acc: 0.7657 - val_loss: 0.5523 - val_acc: 0.7685\n",
            "Epoch 46/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.5721 - acc: 0.7615 - val_loss: 0.5703 - val_acc: 0.7631\n",
            "Epoch 47/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.5505 - acc: 0.7687 - val_loss: 0.5810 - val_acc: 0.7586\n",
            "Epoch 48/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.5529 - acc: 0.7679 - val_loss: 0.5556 - val_acc: 0.7667\n",
            "Epoch 49/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.5438 - acc: 0.7703 - val_loss: 0.5729 - val_acc: 0.7630\n",
            "Epoch 50/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.5448 - acc: 0.7707 - val_loss: 0.5513 - val_acc: 0.7689\n",
            "Epoch 51/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.5337 - acc: 0.7742 - val_loss: 0.5476 - val_acc: 0.7708\n",
            "Epoch 52/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.5449 - acc: 0.7697 - val_loss: 0.5772 - val_acc: 0.7617\n",
            "Epoch 53/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.5373 - acc: 0.7729 - val_loss: 0.5539 - val_acc: 0.7685\n",
            "Epoch 54/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.5294 - acc: 0.7765 - val_loss: 0.5510 - val_acc: 0.7702\n",
            "Epoch 55/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.5295 - acc: 0.7762 - val_loss: 0.5764 - val_acc: 0.7605\n",
            "Epoch 56/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.5623 - acc: 0.7644 - val_loss: 0.5619 - val_acc: 0.7645\n",
            "Epoch 57/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.5375 - acc: 0.7734 - val_loss: 0.5900 - val_acc: 0.7581\n",
            "Epoch 58/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.5296 - acc: 0.7768 - val_loss: 0.5619 - val_acc: 0.7624\n",
            "Epoch 59/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.5218 - acc: 0.7790 - val_loss: 0.5303 - val_acc: 0.7774\n",
            "Epoch 60/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.5218 - acc: 0.7793 - val_loss: 0.5537 - val_acc: 0.7699\n",
            "Epoch 61/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.5206 - acc: 0.7792 - val_loss: 0.5231 - val_acc: 0.7811\n",
            "Epoch 62/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.5125 - acc: 0.7826 - val_loss: 0.5484 - val_acc: 0.7734\n",
            "Epoch 63/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.5113 - acc: 0.7833 - val_loss: 0.5308 - val_acc: 0.7783\n",
            "Epoch 64/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.5230 - acc: 0.7771 - val_loss: 0.5304 - val_acc: 0.7789\n",
            "Epoch 65/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.5079 - acc: 0.7842 - val_loss: 0.5304 - val_acc: 0.7788\n",
            "Epoch 66/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.5043 - acc: 0.7855 - val_loss: 0.5390 - val_acc: 0.7732\n",
            "Epoch 67/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4998 - acc: 0.7871 - val_loss: 0.5303 - val_acc: 0.7769\n",
            "Epoch 68/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4957 - acc: 0.7887 - val_loss: 0.5286 - val_acc: 0.7790\n",
            "Epoch 69/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4938 - acc: 0.7895 - val_loss: 0.5358 - val_acc: 0.7767\n",
            "Epoch 70/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.5009 - acc: 0.7868 - val_loss: 0.5313 - val_acc: 0.7780\n",
            "Epoch 71/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.4968 - acc: 0.7883 - val_loss: 0.5251 - val_acc: 0.7799\n",
            "Epoch 72/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4932 - acc: 0.7897 - val_loss: 0.5274 - val_acc: 0.7808\n",
            "Epoch 73/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4890 - acc: 0.7923 - val_loss: 0.5263 - val_acc: 0.7782\n",
            "Epoch 74/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.4828 - acc: 0.7939 - val_loss: 0.5209 - val_acc: 0.7818\n",
            "Epoch 75/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4815 - acc: 0.7943 - val_loss: 0.5135 - val_acc: 0.7879\n",
            "Epoch 76/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4787 - acc: 0.7948 - val_loss: 0.5190 - val_acc: 0.7825\n",
            "Epoch 77/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.4753 - acc: 0.7965 - val_loss: 0.5569 - val_acc: 0.7740\n",
            "Epoch 78/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4705 - acc: 0.7979 - val_loss: 0.5646 - val_acc: 0.7689\n",
            "Epoch 79/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4821 - acc: 0.7938 - val_loss: 0.5406 - val_acc: 0.7762\n",
            "Epoch 80/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.4689 - acc: 0.7984 - val_loss: 0.5155 - val_acc: 0.7858\n",
            "Epoch 81/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4653 - acc: 0.8010 - val_loss: 0.5218 - val_acc: 0.7850\n",
            "Epoch 82/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4596 - acc: 0.8014 - val_loss: 0.5380 - val_acc: 0.7786\n",
            "Epoch 83/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4589 - acc: 0.8029 - val_loss: 0.5094 - val_acc: 0.7893\n",
            "Epoch 84/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4582 - acc: 0.8029 - val_loss: 0.5198 - val_acc: 0.7850\n",
            "Epoch 85/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4598 - acc: 0.8027 - val_loss: 0.5207 - val_acc: 0.7860\n",
            "Epoch 86/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4705 - acc: 0.7991 - val_loss: 0.5253 - val_acc: 0.7856\n",
            "Epoch 87/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.4585 - acc: 0.8030 - val_loss: 0.5182 - val_acc: 0.7865\n",
            "Epoch 88/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4581 - acc: 0.8033 - val_loss: 0.5429 - val_acc: 0.7801\n",
            "Epoch 89/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4484 - acc: 0.8070 - val_loss: 0.5295 - val_acc: 0.7863\n",
            "Epoch 90/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4425 - acc: 0.8092 - val_loss: 0.5354 - val_acc: 0.7838\n",
            "Epoch 91/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4449 - acc: 0.8077 - val_loss: 0.5287 - val_acc: 0.7860\n",
            "Epoch 92/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.4386 - acc: 0.8105 - val_loss: 0.5269 - val_acc: 0.7885\n",
            "Epoch 93/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4378 - acc: 0.8099 - val_loss: 0.5227 - val_acc: 0.7864\n",
            "Epoch 94/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4433 - acc: 0.8089 - val_loss: 0.5355 - val_acc: 0.7855\n",
            "Epoch 95/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4334 - acc: 0.8130 - val_loss: 0.5253 - val_acc: 0.7911\n",
            "Epoch 96/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4364 - acc: 0.8114 - val_loss: 0.5298 - val_acc: 0.7872\n",
            "Epoch 97/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4340 - acc: 0.8129 - val_loss: 0.5344 - val_acc: 0.7846\n",
            "Epoch 98/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4263 - acc: 0.8153 - val_loss: 0.5350 - val_acc: 0.7865\n",
            "Epoch 99/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4305 - acc: 0.8137 - val_loss: 0.5365 - val_acc: 0.7865\n",
            "Epoch 100/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4355 - acc: 0.8119 - val_loss: 0.5292 - val_acc: 0.7888\n",
            "Epoch 101/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.4240 - acc: 0.8157 - val_loss: 0.5415 - val_acc: 0.7869\n",
            "Epoch 102/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4281 - acc: 0.8149 - val_loss: 0.5355 - val_acc: 0.7871\n",
            "Epoch 103/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4203 - acc: 0.8176 - val_loss: 0.5577 - val_acc: 0.7830\n",
            "Epoch 104/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4237 - acc: 0.8172 - val_loss: 0.5362 - val_acc: 0.7903\n",
            "Epoch 105/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4156 - acc: 0.8195 - val_loss: 0.5681 - val_acc: 0.7819\n",
            "Epoch 106/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4207 - acc: 0.8179 - val_loss: 0.5436 - val_acc: 0.7870\n",
            "Epoch 107/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4153 - acc: 0.8201 - val_loss: 0.5597 - val_acc: 0.7852\n",
            "Epoch 108/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4150 - acc: 0.8203 - val_loss: 0.5437 - val_acc: 0.7898\n",
            "Epoch 109/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4145 - acc: 0.8206 - val_loss: 0.5464 - val_acc: 0.7915\n",
            "Epoch 110/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4128 - acc: 0.8214 - val_loss: 0.5857 - val_acc: 0.7772\n",
            "Epoch 111/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4218 - acc: 0.8189 - val_loss: 0.5424 - val_acc: 0.7876\n",
            "Epoch 112/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4056 - acc: 0.8238 - val_loss: 0.5451 - val_acc: 0.7911\n",
            "Epoch 113/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4047 - acc: 0.8246 - val_loss: 0.5617 - val_acc: 0.7888\n",
            "Epoch 114/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4074 - acc: 0.8235 - val_loss: 0.5526 - val_acc: 0.7918\n",
            "Epoch 115/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4057 - acc: 0.8242 - val_loss: 0.5624 - val_acc: 0.7907\n",
            "Epoch 116/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4043 - acc: 0.8252 - val_loss: 0.5746 - val_acc: 0.7833\n",
            "Epoch 117/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4040 - acc: 0.8243 - val_loss: 0.5658 - val_acc: 0.7933\n",
            "Epoch 118/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.4032 - acc: 0.8255 - val_loss: 0.5755 - val_acc: 0.7883\n",
            "Epoch 119/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.3974 - acc: 0.8276 - val_loss: 0.5659 - val_acc: 0.7923\n",
            "Epoch 120/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.3944 - acc: 0.8289 - val_loss: 0.5729 - val_acc: 0.7911\n",
            "Epoch 121/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.3950 - acc: 0.8283 - val_loss: 0.5710 - val_acc: 0.7914\n",
            "Epoch 122/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.3955 - acc: 0.8285 - val_loss: 0.5664 - val_acc: 0.7919\n",
            "Epoch 123/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.3949 - acc: 0.8289 - val_loss: 0.5689 - val_acc: 0.7885\n",
            "Epoch 124/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.3939 - acc: 0.8292 - val_loss: 0.5705 - val_acc: 0.7908\n",
            "Epoch 125/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.3904 - acc: 0.8307 - val_loss: 0.5799 - val_acc: 0.7898\n",
            "Epoch 126/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.3903 - acc: 0.8309 - val_loss: 0.5827 - val_acc: 0.7843\n",
            "Epoch 127/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.3960 - acc: 0.8290 - val_loss: 0.5723 - val_acc: 0.7908\n",
            "Epoch 128/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.3952 - acc: 0.8292 - val_loss: 0.5765 - val_acc: 0.7876\n",
            "Epoch 129/1000\n",
            "324519/324519 [==============================] - 34s 105us/step - loss: 0.3852 - acc: 0.8330 - val_loss: 0.5761 - val_acc: 0.7931\n",
            "Epoch 130/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.3944 - acc: 0.8292 - val_loss: 0.5857 - val_acc: 0.7928\n",
            "Epoch 131/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.3822 - acc: 0.8340 - val_loss: 0.5811 - val_acc: 0.7917\n",
            "Epoch 132/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.3915 - acc: 0.8311 - val_loss: 0.5923 - val_acc: 0.7914\n",
            "Epoch 133/1000\n",
            "324519/324519 [==============================] - 35s 107us/step - loss: 0.3818 - acc: 0.8346 - val_loss: 0.5762 - val_acc: 0.7944\n",
            "Epoch 134/1000\n",
            "324519/324519 [==============================] - 35s 107us/step - loss: 0.3826 - acc: 0.8344 - val_loss: 0.5952 - val_acc: 0.7925\n",
            "Epoch 135/1000\n",
            "324519/324519 [==============================] - 35s 106us/step - loss: 0.3816 - acc: 0.8348 - val_loss: 0.5805 - val_acc: 0.7921\n",
            "Epoch 136/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.3788 - acc: 0.8359 - val_loss: 0.6217 - val_acc: 0.7791\n",
            "Epoch 137/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.3811 - acc: 0.8349 - val_loss: 0.6005 - val_acc: 0.7934\n",
            "Epoch 138/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.3789 - acc: 0.8359 - val_loss: 0.5792 - val_acc: 0.7919\n",
            "Epoch 139/1000\n",
            "324519/324519 [==============================] - 34s 106us/step - loss: 0.3791 - acc: 0.8357 - val_loss: 0.5997 - val_acc: 0.7934\n",
            "Epoch 140/1000\n",
            "324519/324519 [==============================] - 35s 107us/step - loss: 0.3799 - acc: 0.8355 - val_loss: 0.5907 - val_acc: 0.7918\n",
            "Epoch 141/1000\n",
            "324519/324519 [==============================] - 35s 107us/step - loss: 0.3756 - acc: 0.8370 - val_loss: 0.6267 - val_acc: 0.7787\n",
            "Epoch 142/1000\n",
            "169984/324519 [==============>...............] - ETA: 15s - loss: 0.3749 - acc: 0.8372"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-da77640fcde8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9995\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_smote_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3aZJ1G16xT1"
      },
      "source": [
        "#### Minibatch size:\n",
        "\n",
        "Read [here](https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network)\n",
        "\n",
        "Main takeaway: bigger minibatch size imply greater Learning rate, as shown here:\n",
        "\n",
        "![alt text](https://forums.fast.ai/uploads/default/original/2X/8/88ebf1f95f1fadb0881f3dbc5f517c73713dcba3.PNG)\n",
        "\n",
        "and here:\n",
        "\n",
        "![alt text](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2018/11/Line-Plots-of-Classification-Accuracy-on-Train-and-Test-Datasets-With-Different-Batch-Sizes.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_rQ5e0cSr37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "f1ff1ded-103c-48dc-dd56-e111cfefacb8"
      },
      "source": [
        "model_r.fit( X_smote_train,y_train1,validation_data=[X_smote_test,y_test1],epochs=1000,callbacks=[history],batch_size=2**9) \n",
        "\n",
        "#512 batch size is good for resnet! Now Trying 256"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-5bdfc61d3662>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_r\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mX_smote_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_smote_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#512 batch size is good for resnet! Now Trying 256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    793\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    129\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    132\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected fc10 to have 3 dimensions, but got array with shape (360577, 10)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "384wZbb824dL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "30c31b35-a1c4-42c6-d5b5-9f392b0af901"
      },
      "source": [
        "model_r.evaluate(X_smote_test,y_test1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-48a2a9a2f3f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_r\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_smote_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1286\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1289\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    793\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    129\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    132\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected fc10 to have 3 dimensions, but got array with shape (40065, 10)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he6JaHimNj54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "3c0edab0-42c3-4d71-82b0-0e6ef2a2c17f"
      },
      "source": [
        "print(tgt1.sum())\n",
        "print()\n",
        "#print(yfin_train.sum(axis=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Analysis           2677\n",
            "Backdoor           2329\n",
            "DoS               16353\n",
            "Exploits          44525\n",
            "Fuzzers           24246\n",
            "Generic           58871\n",
            "Normal            93000\n",
            "Reconnaissance    13987\n",
            "Shellcode          1511\n",
            "Worms               174\n",
            "dtype: int64\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYrW6HW0EFc7"
      },
      "source": [
        "model.save_weights('good.hd5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WG7l2UiENPb"
      },
      "source": [
        "files.download('new.hd5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMFoED5kcwzm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "30a346c7-c386-4337-baab-30e862ca47a0"
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAlwgIAUCIsAAClN3ElYQNSgMlwAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOGrgFuSbojr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "79fcfa0d-11a0-4656-d0f1-4597839bfd87"
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd81PX9wPHX+7L3ZiaQMAMiggxR\ncKEowz1QXNVWqbWOX6tWbWu1tra21tk66qCOKoobFStuVBQIiLKXjCRAEkIm2cnn98fnLrkkl+QS\ncgnk3s/HI4+7+36/9/1+viHc+z7r/RFjDEoppRSAo7sLoJRS6tChQUEppVQ9DQpKKaXqaVBQSilV\nT4OCUkqpehoUlFJK1dOgoJSXROQ5Efmzl8fuEJFTD/Y8SnU1DQpKKaXqaVBQSilVT4OC6lGczTa3\nisgPInJARJ4Vkd4i8oGIlIjIxyIS53b8WSKyTkQKReRzERnhtm+siKxyvu9VILTJtc4QkdXO9y4V\nkdEdLPM1IrJVRPaLyEIR6efcLiLykIjkikixiKwRkVHOfTNFZL2zbNkickuHfmFKNaFBQfVE5wPT\ngGHAmcAHwG+BJOzf/I0AIjIMmA/8n3PfIuBdEQkWkWDgbeBFIB54zXlenO8dC8wDfg4kAP8GFopI\nSHsKKiJTgb8Cs4G+wE7gFefu04ATnPcR4zwm37nvWeDnxpgoYBTwaXuuq1RLNCionuifxpgcY0w2\n8CWwzBjznTGmAngLGOs87iLgfWPMR8aYauAfQBhwHDAJCAIeNsZUG2NeB1a4XWMu8G9jzDJjTK0x\n5nmg0vm+9rgUmGeMWWWMqQTuAI4VkVSgGogC0gExxmwwxuxxvq8aGCki0caYAmPMqnZeVymPNCio\nnijH7Xm5h9eRzuf9sN/MATDG1AGZQH/nvmzTOGPkTrfnA4GbnU1HhSJSCKQ439ceTctQiq0N9DfG\nfAr8C3gMyBWRp0Qk2nno+cBMYKeIfCEix7bzukp5pEFB+bPd2A93wLbhYz/Ys4E9QH/nNpcBbs8z\ngXuNMbFuP+HGmPkHWYYIbHNUNoAx5lFjzDhgJLYZ6Vbn9hXGmLOBXthmrgXtvK5SHmlQUP5sATBL\nRE4RkSDgZmwT0FLgG6AGuFFEgkTkPGCi23ufBq4VkWOcHcIRIjJLRKLaWYb5wFUiMsbZH/EXbHPX\nDhGZ4Dx/EHAAqADqnH0el4pIjLPZqxioO4jfg1L1NCgov2WM2QRcBvwT2IftlD7TGFNljKkCzgOu\nBPZj+x/edHtvBnANtnmnANjqPLa9ZfgYuBN4A1s7GQxc7NwdjQ0+Bdgmpnzgfue+y4EdIlIMXIvt\nm1DqoIkusqOUUspFawpKKaXqaVBQSilVT4OCUkqpehoUlFJK1Qvs7gK0V2JioklNTe3uYiil1GFl\n5cqV+4wxSW0dd9gFhdTUVDIyMrq7GEopdVgRkZ1tH6XNR0oppdz4LCiIyDxnyt+1Lew/25neeLWI\nZIjIFF+VRSmllHd8WVN4Dpjeyv5PgKOMMWOAnwLP+LAsSimlvOCzPgVjzBJn+t+W9pe6vYwAOjy1\nurq6mqysLCoqKjp6isNGaGgoycnJBAUFdXdRlFI9ULd2NIvIudgFRnoBszp6nqysLKKiokhNTaVx\nUsuexRhDfn4+WVlZpKWldXdxlFI9ULd2NBtj3jLGpAPnAH9q6TgRmevsd8jIy8trtr+iooKEhIQe\nHRAARISEhAS/qBEppbrHITH6yBizBBgkIokt7H/KGDPeGDM+KcnzMNueHhBc/OU+lVLdo9uCgogM\ncS1gIiJHAyE0rD/b6cqra9lbVE5NraadV0qplvhySOp87EIlw0UkS0R+JiLXisi1zkPOB9aKyGrs\ncoMXGR/m8a6qqSW3pJJqHwSFwsJCHn/88Xa/b+bMmRQWFnZ6eZRSqqN8OfpoThv7/wb8zVfXbyrA\n2exSW9f5cccVFK677rpG22tqaggMbPlXvGjRok4vi1JKHYzDLs1FRwU4nEHBB5WR22+/nW3btjFm\nzBiCgoIIDQ0lLi6OjRs3snnzZs455xwyMzOpqKjgpptuYu7cuUBDyo7S0lJmzJjBlClTWLp0Kf37\n9+edd94hLCys08uqlFKt6XFB4Y/vrmP97uJm240xlFXVEhLkINDRvlazkf2iuevMI1rcf99997F2\n7VpWr17N559/zqxZs1i7dm39sNF58+YRHx9PeXk5EyZM4PzzzychIaHRObZs2cL8+fN5+umnmT17\nNm+88QaXXXZZu8qplFIHq8cFhZa4Ru10xeqjEydObDSP4NFHH+Wtt94CIDMzky1btjQLCmlpaYwZ\nMwaAcePGsWPHDt8XVCmlmuhxQaGlb/TGGNZmF5MUFUyfGN82y0RERNQ///zzz/n444/55ptvCA8P\n56STTvI4zyAkJKT+eUBAAOXl5T4to1JKeXJIzFPoCiJCgMM3Hc1RUVGUlJR43FdUVERcXBzh4eFs\n3LiRb7/9ttOvr5RSnaXH1RRaE+AQfDFNISEhgcmTJzNq1CjCwsLo3bt3/b7p06fz5JNPMmLECIYP\nH86kSZM6vwBKKdVJxIdTA3xi/PjxpukiOxs2bGDEiBFtvndrbgkBDgdpiRFtHnso8/Z+lVLKRURW\nGmPGt3Wc3zQfAThEfNJ8pJRSPYVfBYVAhwYFpZRqjV8FhQANCkop1Sq/DAqHWz+KUkp1Fb8LCgaD\nVhaUUsozvwsK4Ju5Ckop1RP4V1DwUabUjqbOBnj44YcpKyvr1PIopVRH+VdQ8FFNQYOCUqqn8LsZ\nzdD56bPdU2dPmzaNXr16sWDBAiorKzn33HP54x//yIEDB5g9ezZZWVnU1tZy5513kpOTw+7duzn5\n5JNJTEzks88+69RyKaVUe/W8oPDB7bB3jcddIcYwyJk+m/akz+5zJMy4r8Xd7qmzFy9ezOuvv87y\n5csxxnDWWWexZMkS8vLy6NevH++//z5gcyLFxMTw4IMP8tlnn5GY6HF5aqWU6lK+XI5znojkisja\nFvZfKiI/iMgaEVkqIkf5qiwN17SPvhyRunjxYhYvXszYsWM5+uij2bhxI1u2bOHII4/ko48+4rbb\nbuPLL78kJibGd4VQSqkO8mVN4TngX8ALLezfDpxojCkQkRnAU8AxB33VVr7RYwzbs4tIigqlT0zo\nQV/K8yUMd9xxBz//+c+b7Vu1ahWLFi3i97//Paeccgp/+MMffFIGpZTqKJ/VFIwxS4D9rexfaowp\ncL78Fkj2VVlcRASHD2Y1u6fOPv3005k3bx6lpaUAZGdnk5uby+7duwkPD+eyyy7j1ltvZdWqVc3e\nq5RS3e1Q6VP4GfBBSztFZC4wF2DAgAEHdSFfpLpwT509Y8YMLrnkEo499lgAIiMj+e9//8vWrVu5\n9dZbcTgcBAUF8cQTTwAwd+5cpk+fTr9+/bSjWSnV7XyaOltEUoH3jDGjWjnmZOBxYIoxJr+tcx5M\n6myALbklBB7m6bM1dbZSqr28TZ3drTUFERkNPAPM8CYgdIYATZ+tlFIt6rbJayIyAHgTuNwYs7mr\nrquZUpVSqmU+qymIyHzgJCBRRLKAu4AgAGPMk8AfgATgcbFjRWu8qdq0xBiDuMactuJwDwqa4VUp\n5Us+CwrGmDlt7L8auLozrhUaGkp+fj4JCQltBoYAh1BrjNdB5FBijCE/P5/QUN8Mp1VKqUNl9NFB\nSU5OJisri7y8vDaPLamopqi8BkdRKI7DLCiADYDJyT4fvauU8lM9IigEBQWRlpbm1bHzl+/ijoVr\n+OaOqfSNCfNxyZRS6vDiV1lSAWLCggAoKq/u5pIopdShx3+DQpkGBaWUasp/g4LWFJRSqhkNCkop\nper5XVCI1qCglFIt8rugEBUSiAgUa1BQSqlm/C4oOBxCdGiQ1hSUUsoDvwsKYPsVNCgopVRzfhkU\nosMCNSgopZQHfhkUtKaglFKeaVBQSilVz4+DQk13F0MppQ45fhkUosOCKC6v1rUJlFKqCb8MCjFh\nQVTV1lFRXdfdRVFKqUOK3wYF0FnNSinVlM+CgojME5FcEVnbwv50EflGRCpF5BZflcMTDQpKKeWZ\nL2sKzwHTW9m/H7gR+IcPy+CRBgWllPLMZ0HBGLME+8Hf0v5cY8wKoMs/mTUoKKWUZ4dFn4KIzBWR\nDBHJ8GYd5rZoUFBKKc8Oi6BgjHnKGDPeGDM+KSnpoM+nQUEppTw7LIJCZ4sK1aCglFKe+GVQCHAI\nUaGBuqaCUko1EeirE4vIfOAkIFFEsoC7gCAAY8yTItIHyACigToR+T9gpDGm2Fdlcqf5j5RSqjmf\nBQVjzJw29u8Fkn11/bZoUFBKqeb8svkINCgopZQnGhSUUkrV06CglFKqngYFpZRS9fw2KESHBVFV\nU0dFdW13F0UppQ4ZfhsUdFazUko1p0FBg4JSStXToKBBQSml6mlQKNOgoJRSLhoUtKaglFL1NCho\nUFBKqXp+GxSiNSgopVQzfhsUAhxCVEigBgWllHLjt0EBbG1B11RQSqkGfh0UNNWFUko1pkFBg4JS\nStXToKBBQSml6vksKIjIPBHJFZG1LewXEXlURLaKyA8icrSvytISDQpKKdWYL2sKzwHTW9k/Axjq\n/JkLPOHDskBpHmx4D2oq6zfFhGtQUEopdz4LCsaYJcD+Vg45G3jBWN8CsSLS11flYccSePVS2Le5\nflNMWBCVmj5bKaXqdWefQn8g0+11lnNbMyIyV0QyRCQjLy+vY1dLGmEfczfWb3JNYNNhqUopZR0W\nHc3GmKeMMeONMeOTkpI6dpKEwSABkLehfpOmulBKqca6MyhkAylur5Od23wjMMQGhrxN9Zs0KCil\nVGPdGRQWAlc4RyFNAoqMMXt8esWkdMjVmoJSSrUk0FcnFpH5wElAoohkAXcBQQDGmCeBRcBMYCtQ\nBlzlq7LUS0qHje9BdQUEhWpQUEqpJnwWFIwxc9rYb4Bf+ur6HvVKB1MH+Vugz5EaFJRSqonDoqO5\n0zQZgRQdamNicXlNd5VIKaUOKf4VFBKGNBqBFBjgIFLTZyulVD3/CgqBwR5HIGlQUEopy7+CAjQb\ngRStQUEpper5X1DoNQIKttsRSEBMWKDOaFZKKSf/CwpJw+0IJGcOJG0+UkqpBn4YFJwjkJz9CtGh\nGhSUUsrF/4JCkxFIh2RNwRj7o5RSXcz/goJrBJJzrkJMWBDl1bVU1dR1c8HcvH0dLLiiu0uhlPJD\nPpvRfEhLSoecdYBdaAfsrOakqJDuLFWD7V+Awz//aZRS3cv/agrQaATSIZfqomw/FGdDaY42ISml\nupx/BgW3EUjR9UGhqpsL5ZS73j7WVEBFUfeWRSnld/w0KDSMQEpLiABg/Z6SbiyQG2ezFmBrC0op\n1YW8CgoicpOIRDvXPnhWRFaJyGm+LpzPJAyxbfZ5GxiYEE7/2DC+3rKvu0tl5axteF6yt/vKoZTy\nS97WFH5qjCkGTgPigMuB+3xWKl8LDIZ4OwJJRJgyJJGl2/ZRW3cItOHnrIOovva51hSUUl3M26Ag\nzseZwIvGmHVu2w5PScPr5ypMHppIcUUNa7K7uQ2/rtbmZRp8in2tNQWlVBfzNiisFJHF2KDwoYhE\nAW0O7BeR6SKySUS2isjtHvYPFJFPROQHEflcRJLbV/yD0GsEFOyA6nKOG5wAwNdbu7kJqWAHVJfB\ngEkQGKY1BaVUl/M2KPwMuB2YYIwpwy6r2erymSISADwGzABGAnNEZGSTw/4BvGCMGQ3cA/y1HWU/\nOEnOVdj2bSExMoQRfaP5qrv7FVz9CX1GQVRvrSkopbqct0HhWGCTMaZQRC4Dfg+01dYyEdhqjPnR\nGFMFvAKc3eSYkcCnzuefedjvO0np9jHPzmyeMiSBlTsLKK+q7bIiNJOzDsRhyxbZR4OCUqrLeRsU\nngDKROQo4GZgG/BCG+/pD2S6vc5ybnP3PXCe8/m5QJSIJDQ9kYjMFZEMEcnIy8vzsshtqB+B5AwK\nQ5Ooqq1jxY79nXP+jshZZ8sVFGZrCqUaFJRSXcvboFBjjDHYb/L/MsY8BkR1wvVvAU4Uke+AE4Fs\noNlXdWPMU8aY8caY8UlJSZ1wWRqNQAKYkBpHcICDr7qzX2HvGuh9hH0e2QdKtE9BKdW1vA0KJSJy\nB3Yo6vsi4sD2K7QmG0hxe53s3FbPGLPbGHOeMWYs8DvntkIvy3Tw3EYghQcHcvTA2O7rV6gohsKd\nDUEhqjdUlUDVge4pj1LKL3kbFC4CKrHzFfZiP+Dvb+M9K4ChIpImIsHAxcBC9wNEJNEZYADuAOZ5\nXfLO4DYCCWDKkETW7ykmv7SyS4sBNCwR2nuUfYzsYx+1X0Ep1YW8CgrOQPASECMiZwAVxphW+xSM\nMTXA9cCHwAZggTFmnYjcIyJnOQ87CdgkIpuB3sC9HbuNDnIbgQQweUgiAEu35XdpMYCGkUeuoBDl\nDAo6LFUp1YW8TXMxG1gOXAjMBpaJyAVtvc8Ys8gYM8wYM9gYc69z2x+MMQudz183xgx1HnO1MaZr\nv6L3cuVAsv0Ko5NjiQoN7J4mpJx1EBIDMc6pGlFaU1BKdT1vk/b/DjtHIRdARJKAj4HXfVWwLhE/\n2I5AcjbdBDiE4wYn8NXWfRhjEOnCSds562x/guuakVpTUEp1PW/7FByugOCU3473HrpcI5Cc6zWD\n7VfILixnZ35Z15XDmIag4BIeD44grSkopbqUtzWF/4nIh8B85+uLgEW+KVIX65Vuh4I6ufoVvtq6\nj9TEiK4pQ+EuO9LIPSiIQGRvrSkopbqUtx3NtwJPAaOdP08ZY27zZcG6TFI67N9ePwIpLTHCptLu\nyvkKrjUUXJ3MLprqQinVxbxeCNgY8wbwhg/L0j2S0gFjm5D6jUFEmDwkgQ/X5VBbZwhwdEG/giso\nuDq+XSL72GVDlVKqi7RaUxCREhEp9vBTIiLFXVVIn0qeYB+3f1G/afKQRIrKq1nbVam0c9ZCXBqE\nRDbeHtUbSvZ0TRmUUoo2goIxJsoYE+3hJ8oYE91VhfSp2BToMxo2vFe/6bjBDf0KXaJpJ7NLZB8o\nL4CabphMp5TyS4f/CKLOMOJMyFpR336fFBVCep+orulXqCqD/dua9yeATmBTSnU5DQoA6bMAA5s+\nqN90/NBEMnZ0QSrtvI12VrWnmkL9BDYNCkqprqFBAaDXSIhLhY3v12+aPCSRqto6lm7zcW2hfuSR\np+aj3vZRU2grpbqIBgWwcwLSz7CdzRW2/3zSoAT6xoTyr8+2YrOG+0jOOggKtx3NTWmqC6VUF9Og\n4JJ+BtRWwdaPAAgNCuDGU4by3a5CPt6Q28abD0LOWltTcXj4p4hIsiuxaZ+CUqqLaFBwSZkI4YmN\nmpAuHJdMWmIE//hwE7V1PqgteEpv4c4RYAOD1hSUUl1Eg4KLIwCGz4DNi+uHgAYGOPj1tGFsyilh\n4ffZbZygA0r2Qvl+zyOPXDTVhVKqC2lQcDfiTJuDaMeX9ZtmHdmXkX2jeeijLVTV1HXu9VrrZHaJ\n6qM1BaVUl9Gg4C7tRAiKaNSE5HAIt54+nF37y3g1I7Nzr1e/sM7Ilo/RmoJSqgtpUHAXFApDT4WN\ni6CuoVZw0vAkJqTG8c9PtnTuvIWcdRCdDGFxLR8T1QcO5EFtTeddVymlWuDToCAi00Vkk4hsFZHb\nPewfICKfich3IvKDiMz0ZXm8kn6GnReQvbJ+k4hw6+np5JZU8tzSHZ13rb0/tN50BDYomDobGJRS\nysd8FhREJAB4DJgBjATmiEjTdpLfY9duHgtcDDzuq/J4behpdjW2je812jwxLZ6Thifx5BfbKCqv\nPvjrlBfa2cwpE1o/rn4FNu1XUEr5ni9rChOBrcaYH40xVcArwNlNjjGAK7FeDLDbh+XxTlgspB7f\nqF/B5ZbThlNUXs3TS348+OtkZdjH5ImtH6epLpRSXciXQaE/4N4zm+Xc5u5u4DIRycKu5HaDpxOJ\nyFwRyRCRjLy8LmhGSZ8F+VsaLdMJMKp/DGeM7su8r7eTV3KQmUszl9mJaf3HtX6cprpQSnWh7u5o\nngM8Z4xJBmYCL4pIszIZY54yxow3xoxPSkryfanSZ9nHJk1IADefNpzKmjru/3DjwV0jc5mdn9B0\nDYWmXEFBawqqs+RthtXz2z5O+SVfBoVsIMXtdbJzm7ufAQsAjDHfAKFAog/L5J3ofvYbvIcmpLTE\nCK6eksaCjCxW7NjfsfPX1tiO7JRj2j42MBjC4rWmoDrPsifgneugpqq7S6IOQb4MCiuAoSKSJiLB\n2I7khU2O2QWcAiAiI7BB4dAYZpM+y35wFzfv5rjp1KH0jw3jd2+tobq2AxPactdDVal3QQGcE9g6\nUFPY8B78Z6b+51eNFe6yI9qKfTBLXx32fBYUjDE1wPXAh8AG7CijdSJyj4ic5TzsZuAaEfkemA9c\naXyakrQd0s+0j2teb7YrPDiQu886gs05pTz7VQfWUM5cZh9T2uhkdons3f6aQnU5fPAb2Pk17P6u\nfe9VPVvBTvtY1MmTMVWP4NM+BWPMImPMMGPMYGPMvc5tfzDGLHQ+X2+MmWyMOcoYM8YYs9iX5WmX\npGEwcDJkPAt1zSesTRvZm2kje/PIx1vIKihr37mzVtihprEDvDu+IzWF5U83fBN0S9uh/JwxDcGg\nUIOCaq67O5oPbROuhoIdsPVjj7vvPstOPLt74fr2nTdzma0liHh3fFQfm+rC20pUeQF8+QAMOdV2\nZmtQUC6luVBTYZ8X7uresqhDkgaF1ow4036jX/60x939Y8P41bShfLwhh8XrvGzeKcmxgcbb/gSw\nZairhjIvO7a/ehgqiuDUuyF1Cuxapv0KynIPBNp8pDzQoNCagCAYf5VdeCd/m8dDrpqcRnqfKO5e\nuI4DlV7kJ8pabh/bExSi2jFXoSgLlj0Joy+CPkfaoFBTDrtXeX891XMVOvsTQmO0pqA80qDQlnFX\n2rQXK571uDsowMG9545id1EFj3yype3zZS6DgGDoO9r7MrhSXZTsafvYz/9qR5ac/Fv7euBkQGC7\nNiEpGgLBgOM0KCiPNCi0JaoPjDgLVv8Xqg54PGTcwHjmTEzh2a+2s2pXQevny1wO/cZCYEg7yuDl\nBLbcDbD6ZZhwDcQNtNvC47VfQTUo3AXhCdBrhB2I4GEQhfJvGhS8MXGubaNf81qLh9w2PZ3EyGAu\nfPIb/vTeekoqPCTNq6m0w0O9HYrq4m1SvE/ugeBIOP7mxttTp9hgVHOQqTnU4a9wlx31FpsCdTXe\n1T6VX9Gg4I0Bk+y37eXPtDgCKDY8mA9uOoHZ41OY9/V2Tv7HF7y5KotG0y72fA+1Ve3rTwAIDoeQ\n6NZrCju/gU2LYPJNEJHQeJ+rXyFb+xX8Xn1QGNDwWik3GhS8IQITr4GcNbDr2xYPi48I5q/nHck7\nv5xMclwYv17wPRc++Q3rdhfZA1yT1trKjOpJaxPYjIGP/mBrFJOua75/4HGAaBOSv3PNUYgdADGu\noKAjkFRjGhS8deSFdsTG8qfaPHR0cixv/uI4/n7+aLbvO8CZ//yKv/9vI2bXMohLbegjaI/WJrBt\nfN+Oajr5DluraCo8Hvpov4Lfc81RiB1om48AirSmoBrToOCt4AgYcxlsWAglbQ8NdTiE2RNS+PSW\nk7hwXAqPf76Voi1fU92vjUV1WtJSTaGmCj66ExKH2fK1JPV47Vfwd66motgBEBQGEUnafKSa0aDQ\nHhN+ZjvnVj7n9VtiwoL42wWjefC0OGJr9/PEtgR25bczLQY01BSa9mks/zfs/xFO/wsEBLb8/tQp\n9lui2zKjys+45ii4+hNiB2jzkWpGg0J7JAy2qSMy/gO1ztFFxthv3+UFnj+0nc5LtNlWv6wYzNmP\nfcU32/Lbd+2oPrazuLK4YduBffDF32HINBg6rfX3u/oVdL6C/3IFhZiUhketKagmNCi018S5thnn\nH0PhL/3hnnj4cy/4Wyo8MAxevcxzE03mMgiO5P7rLiY+IpjLn13GS8t2en/dSA/Lcn76Zzt34vS/\ntP3+sDg7w1n7FfyXa46Ca2Gn2AF2BnxdB9K/qx6rlfYG5dGQaXYeQNl+CAq3HbtBYfZ5yV74+mF4\n+SK4+CXbD+GSuQz6jyO1VzRv/XIyN87/jt+9tZal2/L5xYmDGdU/pvXruqe6SBoGe9fCqudtkEoa\n5l3ZU4+3WV+rKyAotGP3rw5fruGoLrEDoLYSDuQ2rAWu/J4GhfZyOOCUP7S8P3EYLLweXjwPLl1g\nRyxVlkLOWjj+FgCiQ4N49icTeOSTLTz75Y+8/8MejkmL5+rjB3FKei8cDg/ZU91rCsbAh3fYc594\nm/dlT50C3z4G2Rn2ufIvhbug9xENr2PdhqVqUFBO2nzU2cZeChf8x3boPn8mHMi3z01do0lrAQ7h\n19OG8c1vT+F3M0eQub+Ma17I4JQHv+DFb3ZQVtUkuV59qos9dpLa9iVw0m/tcFNv1c9X+Oqgb1Md\nZurq7Id/7MCGba6+hcJ2NGOqHk+Dgi8ccQ5c/DLkbYLnZtphrADJ45sdGh0axDUnDGLJb07mn3PG\nEh0WxJ3vrOOk+z/n3e93N8yIDomGwDA7+ejD30FSOoz/afvKFRZrE/FpUPA/B3JtU1Gj5iPXXAUd\ngaQa+DQoiMh0EdkkIltF5HYP+x8SkdXOn80iUujL8nSpYafBpa/bjrwVz0DSCPuh3ILAAAdnHtWP\nt687jgU/P5Ze0SHcMP87rpi3nB/zSu2s6qjesOpFKNje9hDUlrjmK1RXHMTNqcNO/RwFt5pCSJQd\ngKDDUpUbnwUFEQkAHgNmACOBOSIy0v0YY8yvnMtwjgH+Cbzpq/J0i7Tj4Yp3IDQWhp7q1VtEhIlp\n8bzzyyncc/YRrN5VyPSHv+TBxZuoi+hth6UOPR2GnNKxMqVOsd8Ys1Z07P3q8OQ+cc2dDktVTfiy\npjAR2GqM+dEYUwW8ApzdyvFzgPk+LE/3SB4PN2+CU+5q19sCHMIVx6byyS0nMmt0Xx79dCuf7Qmk\nTgLh9Hs7Xp4Bx4I4tAnJ39RPXEtpvD12gDYfqUZ8GRT6A+5/bVnObc2IyEAgDfi0hf1zRSRDRDLy\n8vI6vaA+FxRqV3HrgF5RoTzxSs16AAAfhElEQVR00RhevuYY5ofM5meVv+Lq9ws7NisabBNWH+1X\n8DuFuyA8sfEwaXDOat7l/frfqsc7VDqaLwZeN8Z4XPHDGPOUMWa8MWZ8UlJSFxft0HDc4EQe//UV\nTDp9Dku35XPqQ1/w0EebqajuwCIpqVNsAr3yntOFo9rQdI6CS+wAqC7zfv1v1eP5MihkA+511WTn\nNk8upic2HXWy4EAHPz9xMJ/efBKnH9GHRz7ZwrSHvuCj9TmN121oy6jz7BDZl2fbORSq5yvY6Tko\n6LBU1YQvJ6+tAIaKSBo2GFwMXNL0IBFJB+KAb3xYlh6lT0wo/5wzljkTU7jrnXVc80IGY1JiGdIr\nkn4xofSNDaNvTCj9Y8PoHxdGeHCTf+b+4+D8Z+H1q2D+xXDpa3ZWtuqZ6upsv0H6rOb7XIGiKBP6\nH9215VKHJJ8FBWNMjYhcD3wIBADzjDHrROQeIMMY4xy8z8XAK6ZdX3UV2CalRTcdzwvf7OS9H3bz\n1ZZ95JZUUOf2mwxwCEf2j+G4wQlMHpLIuIFxhAYF2LkUtVWYN+dS9sJFfDDqIb7bXca+0kp+Mz2d\nwUnO/Dg1VbDsSTus9tS7bS1DHV5Kc+yKfx6bj1w1BR2BpCw53D6Lx48fbzIyMrq7GIes6to6coor\n2FNUwe7CcrbklPLNj/l8n1lITZ0hOMDB0QNjGd47ik05JQzNeos/Of7N4tpx3Oa4mRoCCQsO4NWf\nH0ta4bfwwW2Qv8Xm3i/Lh3OfgtEXdvdtqvbYtQzmnQaXvGbnz7gzBu4bAEfNgZl/757yqS4hIiuN\nMc1n0DahuY96mKAAB8lx4STHNV6BrbSyhhU79rN06z6Wbsvn1YxMhveJRsZdwXfViZy29l6mjVzA\n5ikP8+tnFrHzsXNJM8sgLg0uWWA7p1++CN6aa9eUGDOnm+5QtZurFhA3sPk+EduvoMNSlZMGBT8R\nGRLIycN7cfLwXh72joJ+Ycji3zO8ZA/vymoq6+DJgEuYNedeUno58ytdsgBemQNv/8IGhqMv93yx\nfVvh+5fBEQTxg+w6FPGD2penSXWepusoNOUalqoUGhSUy3E32JXZPv0zjiPOZddRt/HE/ExenLea\nV+ZOIiXemSZ8zivwyqU2E2xddUP+pbo62Pap7X/Y+hFIgB3hhFvzZFgcxA+2weTon9hvqa0pL4TF\nv7NJBY++Aoae1rHUHv6ucJdt/vO0fjfYfoWdS7u2TOqQpf/DVIMTboXxP4PweIYDL13dj0ue/pZL\nnvmWV+YeS//YMDtK6eKXYcEV8N6voKoMAkNg2b9t30Nkb5u9dfxVNolf4U7I3wb7t9llQ7NXwbs3\nwdZP4KxHbaDwJHsVvHYlFGfbhWE2fwDR/WHclTD2coju23BsXS3kbYRd39q8TiGRMOPv4Ajogl/a\nYaClOQousQOgssgG4Vbycyn/oEFBNebWxDOqfwz/vfoYLn1mGXOe+pY/nDGSk4YnERgUChe9CK9d\nZb/JA/Q7Gs57GkaeA4HBDedLGm5/XOrq4Jt/wSd/hCdXwwXPQsrEhv3G2JFOH/4WInqxcurLVPY6\niuNqV8CKZ+Gze+Hz+yB9JvQ6wk7Cy8poWKY0LM4ujRrZG078jQ9/UZ3AtY5BB2e7e3+dXTY7bkti\n3LKlalDwezr6SLVpdWYhc1/IILekksTIEM47uj8XjEtmWEIIZMyz8x5SJrTvpFkr7TyJoiyY+juY\n/CuoKoGFN8L6t6kaNI07Hdfz6toDiMBdZ4zkyslpttax8j/w3Uv2w7/3ETaopBxjH+PS4K2fw5rX\n4PK3YdCJvvmlHKzM5fCfmbYp7YyHfHedujq4tzdM+gVMu8fzMdkr4emptgboaS6D6hG8HX2kQUF5\npbq2js835fFaRiafbsylps5wVHIM5x2dzMCEcEKDAggJdBASGEBokIPQoAASIoMJCWylCaeiCN79\nP1j3pk3pXZwNBTvZcuSvuXT9RArKa7hh6lDWZBfx0focrjk+jTtmjLAr09VU2WyvIVHNz1tZaj/k\nygvg2i/bv6pYbQ2sf9uuZ33qH73/9lxRDKHRbR9Xmgf/PgFKdkNgKPxqPUQktK+M3ireAw+mw6wH\nYMLVno85sA/uHwzT/waTrvVNOVS30yGpqlMFBTiYNrI300b2Jr+0krdX7+a1jEzuWriuxfeIQJ/o\nUFLiwkmJDyclPoyUuHBSEyMYnBRBbHgMXDAPBp8Mi35DXWgsjw98hH8sjye9TyjP/XQMI/tFU1tn\nuOfddTz95XZ2F1bwwOyjCA0KbtxM5S4kEma/AE+fDG9cbWsM3nRQV5bCdy/CN49DkdtonDMfafu9\n27+EF8+xHeIzH7DLtnpSW2NrSOX74bxn4M2rbc3nhFvavkZHeFpHoanwBLuAk45AUmhQUB2QEBnC\nz6ak8dPJqWzfd4CCsmoqa2qprKmjsto+llfVsqeogsyCMrL2l7N02z72flfRKBlnfEQwgxIjGJR0\nFMPGvsnLq/PZuTmAG6YO5oapQwkOtB+sAQ7h7rOOIDkunHsXbSCnuIKnrxhPXEQLQQGgVzrMehDe\nvha+uA+m/r7lY0tzbUf5imegohBSJsGMv9lMst8+BkfOhtTJLb+/sgTeuc5+sGbMs6/PecJzX8Gn\nf7I1kLMft5MAv3/ZXnfyTb7pW6hPmd1KR7OIM4W2BgWlQUEdBBFhkCsdhhcqa2rJKihnx74D/Jh3\ngB/3lbIt7wCfbsxlQWkVQ3pF8+YVR3FUSvPmGhHhmhMG0S82jF8tWM35TyzluasmMiChhWGWYCfY\n7fwaltxvP+jdFzqqrbZDaH94FTa8Z9NApM+yH86uju9BJ8LGd+1oqWu/sinQPVl8p+00/un/7NDO\nT/5oA8OFzzXOKbXhPfj6YRh3lV3LG2DSdfDSBbD+HTjyAq9/l15ra46CS2yKrsCmAA0KqguFBAYw\nOCmSwUmRnDKi8b6SimoiggNtf0ErZo3uS6/oEK55IYOZj37J9VOHcNXk1Jb7Lmbeb4e3vnmN7V8o\n2WsDwdo3bNqOsHjb5HPMtZA4pPF7gyNsJ/B/z4cvH7Ad4k1t/cQ2/xx3AwyYZH9CY+D9m+GlC2HO\nfNvvkb/NTvrrd7SthbgMPgUShsI3j8Go89ueu9Febc1RcIkdYH9Pyu8dKuspKD8XFRrUZkBwmZAa\nz8JfTmHSoATu+2Ajpz20hMXr9npOHx4UBrOftzWBf46DZ06Blc9D2gl2It7Nm2DWP5oHBJchp9rm\no68egtwNjfdVFMHCGyBxGJzs1jw14Wd2eO7OpfD8WfYb+KuXgSPQ9nUEhjQc63DAMT+H3at8s0Rq\n4a7W+xNcYlJsP4emUvd7GhTUYWlAQjjP/GQ8L/x0IsEBDua+uJLLn13Opr0lzQ9OHGo/pNNOhLP+\nBbdusU07w2e03Fntbvpf7bf9hTfaIZ4u//stlOyBc55s3rQ0+kK4+CXIWWeDUe4GOyej6XKYYJPR\nhcbAt4+363fglbYmrrm4p9BWfk2bj9Rh7YRhSSy66Xhe+nYnD360mZmPfsnU9F5EhQQS4BACA4QA\nhxAgA4hMvIfhAdGMLHaQFmwI8LJmQkQinP4X22md8SxMvAY2/Q9W/xeOvxmSx3l+3/AZcNkbdvb3\nlN/B4KmejwuJtGk/vnnM1io8BY6OqKuz5xtxVtvHuoJCYSb0GtH6sYeylc/ZFCst5eVSbdKgoA57\nQQEOrpycxtlj+vPwx5tZsmUfNXV11NYaao2hts5QU2coraihxrnYRGiQg+G9oxjRN5ojk2OYNqI3\nvaJb6EgGOOpi2xfx8R9hwLG287nXEXDiba0XLu14uHVby0NUXSbOtUFhxdOeJ5mVF8Li30NcqvfD\nV0v32vxU7akptGcFtsJMW94pv2o5XUlXOrDPpnoPCLGd9rpwVIdoUFA9RlxEMH88e1SL+6tq6tia\nW8qGPcVs2FPM+j3FfLhuL6+syOT3b69lYmo8ZxzVj+lH9CEpqqHd3xjDrv1lrBv4G07dfg7myakE\nSS01F79KsHv/QEvaCghgawcjzrT9HSfeZju5XTJXwOs/bRgyGpfq3Uglb+YouET0goBg75uPivfA\n82dCwXb7YXyOD5q+2mv50zapY00FbFpkO+5Vu/k0KIjIdOAR7Mprzxhj7vNwzGzgbmw6ze+NMc2W\n7FSqMwQHOhjZL5qR/RpmHRtj2Jpbyvtr9vDeD3u48+213PXOWiYNSuDYQQlszClhxfb95JZUAnBj\n6IX8mv/yYNUFvP5iPtdP3cWF45MJCuiE7rlJ19mZ1N+/Yjur6+pg6SPwyZ8gpj9c9YF9/s71kJQO\nfVoOgIBbUPCipuBw2M5mb4alHtgHL5wNB/JsrqvVL8GRF9pJiN2lqszWWoaeBjnrYfV8DQod5LM0\nFyISAGwGpgFZ2DWb5xhj1rsdMxRYAEw1xhSISC9jTG5r59U0F8pXjDFszinl/R92894Pe/hx3wH6\nxYQyIS2eCanxTEyLZ0hiOJK1nK8q0njg422sziwkJT6MG6cO5dyx/Qk8mOBgjE3PUVUKP3kX3roW\nfvzMfvCe+YhNt1GSY1NkBIXB3M9ab7ZZcj98+mf43V7vmlJeONuOPrrmk5aPKS+A586E/K1w2evQ\nfzw8OdnO+7ju27aHvvrKimfsMOArF8HWj+18kF9vaH+Kkx6s23MficixwN3GmNOdr+8AMMb81e2Y\nvwObjTHPeHteDQqqKxhjKC6vISa85VnGxhg+25TLgx9tZm12MQMTwhmUGEGAQ3CI7eR2iBAc4OC4\nIYnMOrIvYcFtpPP+4TWb+iI40i5kNP0+my7cff7CrmXw3Cz7zXzOq56bp/ZtgQU/gbJ9cMtm7256\n4Q2w+cOWj68otqk89q6xw3mHnGK37/jKlue4G+C0P3t3rc5UVwv/Gm8D5NWf2ID1r/Ew7U8w+cau\nL88hytug4Mshqf0B97polnObu2HAMBH5WkS+dTY3KdXtRKTVgOA6Zmp6b969fgr/vnwcKXHh5B+o\nYk9RBbv2l7E5p5T1u4v5cus+bnnte475y8fcvXCd52GzLiPPtn0GMSkw93O7LkXTCW0DjrHDZLcs\nhiVN1lWurYYl/4AnJtsEgzP/4f1Nxw6E0hzbV/D1o3YYretLY9UBuxzrnu/tXAtXQAC7VOu4K21H\n+e7vvL9eZ9n4vl2r47gb7O8qcaitwXw/Hw6zhJ+HAl/WFC4Aphtjrna+vhw4xhhzvdsx7wHVwGwg\nGVgCHGmMKWxyrrnAXIABAwaM27mzHSMklOpmxhiWbd/P/OW7+GDNXqpq6zh6QCwXTxxAXHgwWQVl\nZBWU1z/mFxRQ5wgmITqCXlEh9ic6hF5RoRyVEstRyTEI2BnS38+3tYXh02H3atvfkLPGBpcZ90NU\nb+8LWpoHSx+1zS+5zlbemBQbAPK32ZQh5z8Lo85r/t7yQnjsGIhMgms+8/0aEe6emWaD2Y3fNSys\n5GpOmvsF9BvTdWU5hB0uzUdPAsuMMf9xvv4EuN0Y0+LUTm0+Uoez/QeqeHNVFi8v38WPeQfqt4cG\nOUiOCyc5LozkuDBq6wy5xZXklVbWP9Y6h9Om94lizsQBnDMqnpiXZ0LBLjtkdsUzdk7FrAfsSKaD\nUZRlg8OWj+DHL+xaF+c8AWNaGQey4V07c/vUu+0w1a6w61uYd7oNgMfMbdheth8eGG6Xi3VPK+LH\nDoWgEIjtaD4FyMZ2NF9ijFnndsx0bOfzT0QkEfgOGGOMyW/pvBoUVE9gjOH7rCIAkuPCSIgIRlrJ\ne1RXZ9h3oJKP1ucwf/ku1mYXExrk4LLhcHvmtQRWFtplSk/7U6udz0Vl1bzzfTaZ+8uYPqoPRw+I\na/W6gF274kAuxCS3fWOvXmYDyS+WQsLgto8/WK9camswv1rXeBgv2EmDO76CX2/0buZ6D9ftQcFZ\niJnAw9ghqfOMMfeKyD1AhjFmodi/xgeA6UAtcK8x5pXWzqlBQSlYk1XE/BW7WLh6N32rdjAouo5e\nI47nxGFJHDs4gYiQhtHmruarV1dksmjNHipr6gh0CDV1hoEJ4Zwzpj/nju1PamJEK1f0UvEe24zU\ndzRcsdC7ORodtc/ZoXzCLZ5To2/6H8y/SFeUczokgoIvaFBQqsGByhre/2EPH67by9Jt+ZRX1xIU\nIExIjefEYUkY4NUVmWzfd4CokEDOGdufiyakMDAhnP+t3cvbq7NZui0fY2DsgFjOHN2PCanxpPeN\n6vjci5XP2Rnf8YNg9EUwerZ93tne/T9Y/TL8ai1E9mq+v7YaHhxhl2q9+KXOv/5hRoOCUn6msqaW\njB0FLNmcxxeb89joHOU0ITWOiycMYGYLQ2L3FJXzzurdvLUqm0059j0hgQ6O6BfNmJQ4xgyIZVS/\naOIjgokMCWx7LoYxdo3s7160K9Jh7Afz6IvgiHMhPL7t91eX2RFPVaUQGmubxNybuUrz4OFR9pxn\nPdryuf73W1j+lB1m29Z13W1eDJv/B6feZZMV9gAaFJTyc3uLKqiqqWt9ISI3xhiyCspZnVnI95mF\nrM4sZE12EZU1dY2OiwgOIDosiKjQQOLCgxmTEsuEVDvBr9kw3qIsGyC+fxXyNthkdSFRNo14/U+A\n/amusEGgqhRM42sSEgPxqXa4blwqFOy0s7+vz7BDUFv8JayBJ6c074huzdo34I1rwNRC71Fw6WsQ\n3c+79x7CNCgopQ5adW0dm/aWsGlvCUXl1ZRU1FBcUU2x83lOSQXrsoupqq1DBIb3jmJiWjzHpCVw\n0vCkhr4NY2DvD3aEUkWRnZhXV+v8qbE/QaF20l5wpM0cGxxpO4/L9kPBjoafwp12fYwRZ8JF/21U\n3to6g0No3Hn+xBS7Rvfcz9u+4e9egoXX25X6Jv3CDvsNjbWztw/n7LFoUFBKdZGK6lpWZxayYvt+\nlu/Yz8qdBZRV1RIeHMAZo/sye3wK4wa2PsqpvKqW0CBH2yOhwOaEKt0L4QkQGIIxhtWZhbyxKot3\nv99DXHgQ950/mkmDEuzx3zwGH/4Wrltm1+5uyfKnYdEtMOhk2zkdHG4n6710oU2yd/HLdqLeYUqD\nglKqW9TU1rFyZwFvrMrivR/2UFZVy6DECC4cn8K5Y/tTUV3rlqm2hI17i8kqKCcuPKg+x9SE1HiO\n6Bfdav/FnqJy3lyVzZurstiWd4CQQAfTRvZmTXYRO/PLuOLYgdw2PZ2I6v3wQLqddX36Xzyvtf31\no/DRnTBshnNtbbdjCnbadbQLdsC5//Y8ee8woEFBKdXtDlTW8P6aPbyWkcmKHQWN9jkE0hIjGNE3\nmqG9osgsKGPFjv3szC8DIDw4gKMHxBEXEUxldS2VNXVUOB/Lq2rZnFuCMbYj/fyjk5k5ui/RoUGU\nVdVw/4ebeG7pDvrHhvH380dz3KqbbR9EQLBdJ3vAJLsuRspEW0P4/C828eD5z3iejV22H+bPgcxl\ndnLeMdd6Di7eKi+ArJX2HJF97OipkKjOX6PbjQYFpdQh5ce8Uj5cl0N8RBAj+kYzrHcUoUHNR0Pl\nFFewfPt+VuzYz4odBVRU1xIS6CAkKMA+BjoICQxgZL9ozmtlfsWKHfv5zes/sH3fAX4yoTe3DdtN\n+J4Vdhb07u/sAkQuR82xS7UGtLKaQHU5vHmN7RcJjoL0mXY01eCpjdfd9qS2BrJXwrZP7U92RvPO\n9KBwGxwi+9hU6TEpdp2NmAHOxxTb19JBGhSUUn6vvKqWBxZv4tmvt2OMHWobHRZEYkgtRwduZ6zZ\nQGhkDAkn38AxgxJxtLVEa10dbP8c1r7p7DQvtCOj0mfBsNNsh3pFkd1eUWR/SvbCjq+hsgjEYWsq\ng6fa/glTB6W5to+kNNceW5pjFzsqym4cuAAm3+R5ZT4vaFBQSimn7zML+XJLntvoKedjRQ1bckoo\nq6qlb0woZx3Vj7PH9GdE36i2O71rq+HHz2HdW7DhPfuh784RaEcuhcfb5qrBp0DaCd7Pl3B1qBdm\n2iBRuAv6je3wYkYaFJRSygtlVTV8tD6Hd1bvZsnmPGrqDMN6RzL9iD4M7hVJSnw4A+LDW89PVVMJ\nOWttE1BojP0JCvdpH0F7aVBQSql22n+givd/2M3bq3ezcmfjjvGI4ABS4sNJTbCd4yP7RXNEv2j6\nxoR6DBbGGEoqaygur6Z/bJh3w219SIOCUkodhIrqWrIKyti1v4yd+fYxc38ZP+YdYHv+gfr1e+LC\ngxjZL5q0xAiKymvIKa4gt7iCnOJKyqtrAegfG8a0kb05/Yg+TEiNO7hlWztIg4JSSvnIgcoaNu4t\nYf3uItbtLmb9nmJ27DtAfEQwvaJD6R0dSu+oEHpHhxIa5OCLzfv4ckselTV1xIUHceoIGyCOGRRP\nVGjXLEikQUEppQ4hBypr+GJzHh+u28unG3IpqazBIZDeJ5oJqXGMS41nQmocfWPCfHJ9DQpKKXWI\nqqqpY3l9WpD9fLerkLKqhqamEX2jGNwrkiFJkQzpZX8OtkbhbVBoZaaGUkopXwgOdDBlaCJThiYC\nNjXIhj0lZOy0uaM255TwxeY8qmsbvrT3jg7hmuMHcfXxPlibwo0GBaWU6maBAQ6OTI7hyOQYrpqc\nBthAsWt/GVtzS9mSW8q23FKSotqYOd0ZZfHlyZ1rMD+CXY7zGWPMfU32Xwncj13DGeBfxphnfFkm\npZQ6HAQGOBiUFMmgpEhOO6ILr+urE4tIAPAYMA3IAlaIyEJjzPomh75qjLneV+VQSinlPV8Olp0I\nbDXG/GiMqQJeAc724fWUUkodJF8Ghf5AptvrLOe2ps4XkR9E5HURSfF0IhGZKyIZIpKRl5fni7Iq\npZTCt0HBG+8CqcaY0cBHwPOeDjLGPGWMGW+MGZ+UlNSlBVRKKX/iy6CQDbh/80+moUMZAGNMvjGm\n0vnyGWCcD8ujlFKqDb4MCiuAoSKSJiLBwMXAQvcDRKSv28uzgA0+LI9SSqk2+Gz0kTGmRkSuBz7E\nDkmdZ4xZJyL3ABnGmIXAjSJyFlAD7Aeu9FV5lFJKtU3TXCillB/osbmPRCQP2NnBtycC+zqxOIcD\nvWf/oPfsHw7mngcaY9ocqXPYBYWDISIZ3kTKnkTv2T/oPfuHrrjn7h6SqpRS6hCiQUEppVQ9fwsK\nT3V3AbqB3rN/0Hv2Dz6/Z7/qU1BKKdU6f6spKKWUaoUGBaWUUvX8JiiIyHQR2SQiW0Xk9u4ujy+I\nyDwRyRWRtW7b4kXkIxHZ4nyM684ydjYRSRGRz0RkvYisE5GbnNt77H2LSKiILBeR7533/Efn9jQR\nWeb8G3/VmV6mxxCRABH5TkTec77u6fe7Q0TWiMhqEclwbvP537VfBAW3BX9mACOBOSIysntL5RPP\nAdObbLsd+MQYMxT4xPm6J6kBbjbGjAQmAb90/tv25PuuBKYaY44CxgDTRWQS8DfgIWPMEKAA+Fk3\nltEXbqJxfrSefr8AJxtjxrjNTfD537VfBAX8ZMEfY8wSbA4pd2fTkJL8eeCcLi2Ujxlj9hhjVjmf\nl2A/NPrTg+/bWKXOl0HOHwNMBV53bu9R9ywiycAsbDZlRETowffbCp//XftLUPB2wZ+eqLcxZo/z\n+V6gd3cWxpdEJBUYCyyjh9+3syllNZCLXYtkG1BojKlxHtLT/sYfBn4D1DlfJ9Cz7xdsoF8sIitF\nZK5zm8//rn2WJVUdeowxRkR65BhkEYkE3gD+zxhTbL9IWj3xvo0xtcAYEYkF3gLSu7lIPiMiZwC5\nxpiVInJSd5enC00xxmSLSC/gIxHZ6L7TV3/X/lJTaHPBnx4sx7VuhfMxt5vL0+lEJAgbEF4yxrzp\n3Nzj7xvAGFMIfAYcC8SKiOuLXk/6G58MnCUiO7BNv1OBR+i59wuAMSbb+ZiLDfwT6YK/a38JCm0u\n+NODLQR+4nz+E+CdbixLp3O2LT8LbDDGPOi2q8fet4gkOWsIiEgYMA3bl/IZcIHzsB5zz8aYO4wx\nycaYVOz/3U+NMZfSQ+8XQEQiRCTK9Rw4DVhLF/xd+82MZhGZiW2XdC34c283F6nTich84CRset0c\n4C7gbWABMACbcny2MaZpZ/RhS0SmAF8Ca2hob/4ttl+hR963iIzGdjIGYL/YLTDG3CMig7DfpOOB\n74DL3Ja77RGczUe3GGPO6Mn367y3t5wvA4GXjTH3ikgCPv679pugoJRSqm3+0nyklFLKCxoUlFJK\n1dOgoJRSqp4GBaWUUvU0KCillKqnQUGpLiQiJ7myfCp1KNKgoJRSqp4GBaU8EJHLnGsWrBaRfzsT\n0JWKyEPONQw+EZEk57FjRORbEflBRN5y5bgXkSEi8rFz3YNVIjLYefpIEXldRDaKyEvinqhJqW6m\nQUGpJkRkBHARMNkYMwaoBS4FIoAMY8wRwBfYGeMALwC3GWNGY2dWu7a/BDzmXPfgOMCV3XIs8H/Y\ntT0GYXP7KHVI0CypSjV3CjAOWOH8Eh+GTTxWB7zqPOa/wJsiEgPEGmO+cG5/HnjNmbemvzHmLQBj\nTAWA83zLjTFZztergVTgK9/fllJt06CgVHMCPG+MuaPRRpE7mxzX0Rwx7vl5atH/h+oQos1HSjX3\nCXCBM4+9a13cgdj/L66snJcAXxljioACETneuf1y4AvnKnBZInKO8xwhIhLepXehVAfoNxSlmjDG\nrBeR32NXvXIA1cAvgQPAROe+XGy/A9gUxk86P/R/BK5ybr8c+LeI3OM8x4VdeBtKdYhmSVXKSyJS\naoyJ7O5yKOVL2nyklFKqntYUlFJK1dOaglJKqXoaFJRSStXToKCUUqqeBgWllFL1NCgopZSq9/8F\nepURi4Y+nQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEsl8EvMSeB4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6fb80b69-63ce-4afe-ef25-864e04800c68"
      },
      "source": [
        "model.evaluate(X_smote_test,y_test1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40065/40065 [==============================] - 15s 378us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5523409330138594, 0.7723698989142643]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa9mNNJ3M-Mi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "501c5321-390c-4d45-c6ed-2fdb423d2715"
      },
      "source": [
        "y_smote.sum(axis=0)/y_smote.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.07487982, 0.07487982, 0.07487982, 0.11113413, 0.06051787,\n",
              "       0.14694166, 0.23212744, 0.07487982, 0.07487982, 0.07487982])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMDh_MY9I2Vy"
      },
      "source": [
        "# Other models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtKZKwA9rojU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0b1a9f9c-7f63-49ce-90fe-1ba6dcb51a5f"
      },
      "source": [
        "clf = RandomForestClassifier(n_estimators = 300, criterion='entropy',min_samples_split=10,bootstrap=True, max_depth=None, max_features='auto', max_leaf_nodes=None, min_impurity_split=None, min_samples_leaf=1, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=True)\n",
        "#ov = OneVsRestClassifier(clf)\n",
        "#clf_fin = AdaBoostClassifier(base_estimator = clf, n_estimators=100,algorithm='SAMME.R',)\n",
        "from sklearn.model_selection import cross_val_score\n",
        "print(cross_val_score(clf,X,tgt2,cv=10))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.98047966 0.96914778 0.84150885 0.84029961 0.93689603 0.93778864\n",
            " 0.96138472 0.98071176 0.91248496 0.89878527]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbbgNwT3r1nv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "9e44f0b1-f7d2-41a9-892b-b06826cd24ca"
      },
      "source": [
        "pred = clf.predict(x_test4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFittedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-92cd60a08a7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \"\"\"\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0mcorresponds\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \"\"\"\n\u001b[0;32m--> 586\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'estimators_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFittedError\u001b[0m: This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this method."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQx0-yJUr9Xm"
      },
      "source": [
        "print(\"Accuracy score is of {}\".format(accuracy_score(pred,y_test4)))\n",
        "print(\"#############################################\")\n",
        "print(\"Recall score is of {}\".format(recall_score(pred,y_test4,average='weighted')))\n",
        "print(\"#############################################\")\n",
        "print(\"Precision score is of {}\".format(precision_score(pred,y_test4,average='weighted')))\n",
        "print(\"#############################################\")\n",
        "print(\"F1 score is of {}\".format(f1_score(pred,y_test4,average='weighted')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6zeu8eWhd2P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "6e0a6454-7a07-4ee6-c866-497b9a2f89f3"
      },
      "source": [
        "one_to_left = st.beta(10, 1)  \n",
        "from_zero_positive = st.expon(0.01, 50)\n",
        "\n",
        "params = { \n",
        "    \"base_score\":st.gamma(0.001,0.8),\n",
        "    \"n_estimators\": st.randint(3, 400),\n",
        "    \"max_depth\": st.randint(0.1,40),\n",
        "    \"learning_rate\": st.uniform(0.0005, 0.4),\n",
        "    \"colsample_bytree\": one_to_left,\n",
        "    \"subsample\": one_to_left,\n",
        "    \"gamma\": st.uniform(0, 100),\n",
        "    'reg_alpha': from_zero_positive,\n",
        "    'reg_lambda':from_zero_positive,\n",
        "    \"min_child_weight\": from_zero_positive\n",
        "}\n",
        "\n",
        "\n",
        "reg_mod = XGBClassifier(objective='binary:logistic')\n",
        "\n",
        "gs = RandomizedSearchCV(reg_mod, params)  \n",
        "gs.fit(x_train4, y_train4)  \n",
        "gs.best_score_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9402778721050946"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6X0wteDuFZd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "ede0d41d-a108-420c-fb95-3b984dd515e7"
      },
      "source": [
        "#grid_param={'estimator__lambda':[0.01,0.03,0.09,0.1,0.5,0.9,3,5],'estimator__max_depth':[100,50,10]}\n",
        "\n",
        "clf2 = gs.best_estimator_\n",
        "#grid = GridSearchCV(cf2,param_grid=grid_param)\n",
        "clf2.fit(x_train4,y_train4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.8, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=0.9638270681019763,\n",
              "              gamma=7.388721885677018, learning_rate=0.02581263729031337,\n",
              "              max_delta_step=0, max_depth=26,\n",
              "              min_child_weight=32.42583138456334, missing=None,\n",
              "              n_estimators=202, n_jobs=1, nthread=None,\n",
              "              objective='binary:logistic', random_state=0,\n",
              "              reg_alpha=18.895987097349014, reg_lambda=16.71632492595375,\n",
              "              scale_pos_weight=1, seed=None, silent=None,\n",
              "              subsample=0.8496802051481626, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Y5QIQuJ0-x_"
      },
      "source": [
        "pred1 = clf2.predict(x_test4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo4LdYyU1olL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "1309e959-c19a-43d0-a9f0-ad3d47597450"
      },
      "source": [
        "print(\"CV Accuracy score is of {}\".format(accuracy_score(pred1,y_test4)))\n",
        "print(\"#############################################\")\n",
        "print(\"CV Recall score is of {}\".format(recall_score(pred1,y_test4,average='weighted')))\n",
        "print(\"#############################################\")\n",
        "print(\"CV Precision score is of {}\".format(precision_score(pred1,y_test4,average='weighted')))\n",
        "print(\"#############################################\")\n",
        "print(\"CV F1 score is of {}\".format(f1_score(pred1,y_test4,average='weighted')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV Accuracy score is of 0.9439798195401183\n",
            "#############################################\n",
            "CV Recall score is of 0.9439798195401183\n",
            "#############################################\n",
            "CV Precision score is of 0.9438772384557506\n",
            "#############################################\n",
            "CV F1 score is of 0.9439112207043515\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5kC0F1Q_ufo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "8ffff245-e618-418d-c278-d37c90824989"
      },
      "source": [],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.8, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=0.9814727895335547,\n",
              "              gamma=15.142202866731115, learning_rate=0.04469975181599223,\n",
              "              max_delta_step=0, max_depth=11, min_child_weight=23.2481498789006,\n",
              "              missing=None, n_estimators=386, n_jobs=1, nthread=None,\n",
              "              objective='binary:logistic', random_state=0,\n",
              "              reg_alpha=20.812716363943416, reg_lambda=178.73896746984744,\n",
              "              scale_pos_weight=1, seed=None, silent=None,\n",
              "              subsample=0.9429914304871988, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    }
  ]
}